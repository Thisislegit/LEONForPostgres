{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 示例数据\n",
    "models = [\"Tree_conv\", \"Transformer\", \"Tree_transformer\"]\n",
    "categories = [\"Analysis\", \"Encoding\", \"Inference\"]\n",
    "\n",
    "data = {\n",
    "    \"Tree_conv\": {\n",
    "        \"Analysis\": 1.699415797899809e-05,\n",
    "        \"Encoding\": 0.32585111174058884,\n",
    "        \"Inference\": 0.011281246731840956\n",
    "    },\n",
    "    \"Transformer\": {\n",
    "        \"Analysis\": 1.5631804619756436e-05,\n",
    "        \"Encoding\": 0.18211924681502545,\n",
    "        \"Inference\": 0.004838372474140837\n",
    "    },\n",
    "    \"Tree_transformer\": {\n",
    "        \"Analysis\": 1.7068361274007607e-05,\n",
    "        \"Encoding\": 0.3275578327122506,\n",
    "        \"Inference\": 0.020449148399678294\n",
    "    }\n",
    "}\n",
    "\n",
    "# 绘制柱状图\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "bar_width = 0.2\n",
    "bar_positions = np.arange(len(models))\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    values = [data[model][category] for model in models]\n",
    "    ax.bar(bar_positions + i * bar_width, values, bar_width, label=category)\n",
    "\n",
    "# 设置坐标轴标签和标题\n",
    "ax.set_xticks(bar_positions + bar_width)\n",
    "ax.set_xticklabels(models)\n",
    "ax.set_yscale('log')  # 设置纵坐标为对数坐标\n",
    "ax.set_ylabel('Log Scale')\n",
    "\n",
    "# 添加图例\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig('./bar_chart.svg', format='svg', bbox_inches='tight')\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_scientific_notation(number):\n",
    "    # 将科学计数法转为字符串\n",
    "    str_number = \"{:e}\".format(number)\n",
    "    # 分离有效数字和指数部分\n",
    "    mantissa, exponent = str_number.split('e')\n",
    "    # 将有效数字转为浮点数，以便进行格式化\n",
    "    mantissa = 9.994 if float(mantissa) >= 9.995 else float(mantissa)\n",
    "    # 限制有效位数为两位\n",
    "    mantissa = format(mantissa, '.2f')\n",
    "    # 将指数部分转为整数\n",
    "    exponent = int(exponent)\n",
    "    # 限制指数的范围在[-9, 9]之间\n",
    "    exponent = max(-9, min(9, exponent))\n",
    "    # 输出格式化后的结果\n",
    "    result = \"{},{},{:d}\".format(mantissa, '1' if exponent >= 0 else '0', abs(exponent))\n",
    "    return result\n",
    "\n",
    "# 示例\n",
    "number = 9.998\n",
    "result = format_scientific_notation(number)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# 生成示例数据\n",
    "\n",
    "\n",
    "with open('./pg.txt','rb') as f:\n",
    "    y1_values = pickle.load(f)\n",
    "with open('./tf.txt','rb') as f:\n",
    "    y2_values = pickle.load(f)\n",
    "\n",
    "num_data_points = len(y1_values)\n",
    "x_values = np.linspace(0, len(y1_values), len(y1_values))\n",
    "\n",
    "# 绘制折线图\n",
    "plt.plot(x_values, y1_values, label='pg')\n",
    "plt.plot(x_values, y2_values, label='tf')\n",
    "\n",
    "# 设置坐标轴标签和标题\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title('Line Chart with Two Labels')\n",
    "\n",
    "# 添加图例\n",
    "plt.legend()\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# 生成示例数据\n",
    "with open('./log/pg10.txt', 'rb') as f:\n",
    "    y1_values = pickle.load(f)\n",
    "with open('./log/tf10.txt', 'rb') as f:\n",
    "    y2_values = pickle.load(f)\n",
    "\n",
    "# y1_values = [894, 892, 902, 859, 865, 881, 892, 845, 866, 869, 885, 898, 891, 887, 880, 882, 879, 876, 825, 816, 878, 872, 875, 861, 870, 870, 871, 861, 904, 883, 889, 872, 880, 872, 889, 883, 904, 861, 871, 870]\n",
    "\n",
    "# y2_values = [990, 561, 533, 517, 1019, 561, 1026, 568, 541, 978, 552, 567, 1008, 428, 1013, 525, 513, 516, 483, 1448, 539, 530, 432, 946, 1441, 499, 2623, 980, 1000, 488, 620, 507, 947, 505, 620, 488, 1000, 980, 2623, 499]\n",
    "num_data_points = len(y1_values)\n",
    "x_labels = range(0, len(y1_values), 1)\n",
    "\n",
    "# 设置图形大小\n",
    "plt.figure(figsize=(30, 5))\n",
    "\n",
    "# 绘制折线图，设置线的透明度为50%\n",
    "plt.plot(x_labels, y1_values, label='pg', alpha=0.5)\n",
    "plt.plot(x_labels, y2_values, label='tf', alpha=0.5)\n",
    "\n",
    "# 设置坐标轴标签和标题\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title('Line Chart with Two Labels')\n",
    "\n",
    "# 设置 x 轴刻度和标签\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# 添加图例\n",
    "plt.legend(fontsize='large')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n",
    "print(len(y1_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# 生成示例数据\n",
    "with open('./log/pg1.txt', 'rb') as f:\n",
    "    y1_values = pickle.load(f)\n",
    "with open('./log/tf1.txt', 'rb') as f:\n",
    "    y2_values = pickle.load(f)\n",
    "\n",
    "num_data_points = len(y1_values)\n",
    "x_labels =  ['1a', '1b', '1c', '1d', '2a', '2b', '2c', '2d', '3a', '3b', '3c', '4a',\n",
    "             '4b', '4c', '5a', '5b', '5c', '6a', '6b', '6c', '6d', '6e', '6f', '7a', \n",
    "             '7b', '7c', '8a', '8b', '8c', '8d', '9a', '9b', '9c', '9d', '10a', '10b', \n",
    "             '10c', '11a', '11b', '11c', '11d', '12a', '12b', '12c', '13a', '13b', '13c', \n",
    "             '13d', '14a', '14b', '14c', '15a', '15b', '15c', '15d', '16a', '16b', '16c',\n",
    "             '16d', '17a', '17b', '17c', '17d', '17e', '17f', '18a', '18b', '18c', '19a',\n",
    "             '19b', '19c', '19d', '20a', '20b', '20c', '21a', '21b', '21c', '22a', '22b']\n",
    "\n",
    "# 设置图形大小\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# 绘制柱状图\n",
    "bar_width = 0.4\n",
    "bar_positions1 = np.arange(len(x_labels))\n",
    "bar_positions2 = bar_positions1 + bar_width\n",
    "\n",
    "plt.bar(bar_positions1, y1_values, width=bar_width, label='pg', alpha=0.5)\n",
    "plt.bar(bar_positions2, y2_values, width=bar_width, label='tf', alpha=0.5)\n",
    "\n",
    "# 设置坐标轴标签和标题\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title('Bar Chart with Two Labels')\n",
    "\n",
    "# 设置 x 轴刻度和标签\n",
    "plt.xticks(bar_positions1 + bar_width / 2, x_labels, rotation=45, ha='right')\n",
    "\n",
    "# 添加图例\n",
    "plt.legend(fontsize='large')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# 生成示例数据\n",
    "with open('./log/pg5.txt', 'rb') as f:\n",
    "    y1_values = pickle.load(f)\n",
    "with open('./log/tf5.txt', 'rb') as f:\n",
    "    y2_values = pickle.load(f)\n",
    "\n",
    "num_data_points = len(y1_values)\n",
    "x_labels =  ['1a', '1b', '1c', '1d', '2a', '2b', '2c', '2d', '3a', '3b', '3c', '4a',\n",
    "             '4b', '4c', '5a', '5b', '5c', '6a', '6b', '6c', '6d', '6e', '6f', '7a', \n",
    "             '7b', '7c', '8a', '8b', '8c', '8d', '9a', '9b', '9c', '9d', '10a', '10b', \n",
    "             '10c', '11a', '11b', '11c', '11d', '12a', '12b', '12c', '13a', '13b', '13c', \n",
    "             '13d', '14a', '14b', '14c', '15a', '15b', '15c', '15d', '16a', '16b', '16c',\n",
    "             '16d', '17a', '17b', '17c', '17d', '17e', '17f', '18a', '18b', '18c', '19a',\n",
    "             '19b', '19c', '19d', '20a', '20b', '20c', '21a', '21b', '21c', '22a', '22b',\n",
    "             '22c', '22d', '23a', '23b', '23c', '24a', '24b', '25a', '25b', '25c', '26a', \n",
    "             '26b', '26c', '27a', '27b', '27c', '28a', '28b', '28c']\n",
    "\n",
    "# 设置图形大小\n",
    "plt.figure(figsize=(25, 5))\n",
    "\n",
    "# 绘制柱状图，设置线的透明度为50%\n",
    "bar_width = 0.4\n",
    "bar_positions1 = np.arange(len(x_labels))\n",
    "bar_positions2 = bar_positions1 + bar_width\n",
    "\n",
    "plt.bar(bar_positions1, y1_values, width=bar_width, label='pg', alpha=0.5)\n",
    "plt.bar(bar_positions2, y2_values, width=bar_width, label='tf', alpha=0.5)\n",
    "\n",
    "# 设置坐标轴标签和标题\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis (log scale)')\n",
    "plt.title('Bar Chart with Two Labels (log scale)')\n",
    "\n",
    "# 设置 x 轴刻度和标签\n",
    "plt.xticks(bar_positions1 + bar_width / 2, x_labels, rotation=45, ha='right')\n",
    "\n",
    "# 设置 y 轴为对数刻度\n",
    "# plt.yscale('log')\n",
    "\n",
    "# 添加图例\n",
    "plt.legend(fontsize='large')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# 生成示例数据\n",
    "with open('./log/pg2.txt', 'rb') as f:\n",
    "    y1_values = pickle.load(f)\n",
    "with open('./log/tf2.txt', 'rb') as f:\n",
    "    y2_values = pickle.load(f)\n",
    "# with open('./log/tf3.txt', 'rb') as f:\n",
    "#     y3_values = pickle.load(f)\n",
    "y3_values = [4197.779, 6310.663, 2685.923, 6786.553]\n",
    "y3_values += [0] * (len(y2_values) - len(y3_values))\n",
    "num_data_points = len(y1_values)\n",
    "x_labels =  ['1a', '1b', '1c', '1d', '2a', '2b', '2c', '2d', '3a', '3b', '3c', '4a',\n",
    "             '4b', '4c', '5a', '5b', '5c', '6a', '6b', '6c', '6d', '6e', '6f', '7a', \n",
    "             '7b', '7c', '8a', '8b', '8c', '8d', '9a', '9b', '9c', '9d', '10a', '10b', \n",
    "             '10c', '11a', '11b', '11c', '11d', '12a', '12b', '12c', '13a', '13b', '13c', \n",
    "             '13d', '14a', '14b', '14c', '15a', '15b', '15c', '15d', '16a', '16b', '16c',\n",
    "             '16d', '17a', '17b', '17c', '17d', '17e', '17f', '18a', '18b', '18c', '19a',\n",
    "             '19b', '19c', '19d', '20a', '20b', '20c', '21a', '21b', '21c', '22a', '22b',\n",
    "             '22c', '22d', '23a', '23b', '23c', '24a', '24b', '25a', '25b', '25c', '26a', \n",
    "             '26b', '26c', '27a', '27b', '27c', '28a', '28b', '28c']\n",
    "\n",
    "# 设置图形大小\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 绘制柱状图，设置线的透明度为50%\n",
    "bar_width = 0.2\n",
    "bar_positions1 = np.arange(len(x_labels))\n",
    "bar_positions2 = bar_positions1 + bar_width\n",
    "bar_positions3 = bar_positions2 + bar_width\n",
    "\n",
    "plt.bar(bar_positions1, y1_values, width=bar_width, label='pg', alpha=0.5)\n",
    "plt.bar(bar_positions2, y2_values, width=bar_width, label='tf_par', alpha=0.5)\n",
    "plt.bar(bar_positions3, y3_values, width=bar_width, label='tf_no', alpha=0.5)\n",
    "\n",
    "# 设置坐标轴标签和标题\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title('Bar Chart with Two Labels (log scale)')\n",
    "\n",
    "# 设置 x 轴刻度和标签\n",
    "plt.xticks(bar_positions1 + bar_width / 2, x_labels, rotation=45, ha='right')\n",
    "\n",
    "# 设置 y 轴为对数刻度\n",
    "# plt.yscale('log')\n",
    "\n",
    "# 添加图例\n",
    "plt.legend(fontsize='large')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# # 生成示例数据\n",
    "# with open('./log/pg5.txt', 'rb') as f:\n",
    "#     a = pickle.load(f)\n",
    "# with open('./log/tf5.txt', 'rb') as f:\n",
    "#     b = pickle.load(f)\n",
    "a = [894, 892, 902, 859, 865, 881, 892, 845, 866, 869, 885, 898, 891, 887, 880, 882, 879, 876, 825, 816, 878, 872, 875, 861, 870, 870, 871, 861, 904, 883, 889, 872, 880, 872, 889, 883, 904, 861, 871, 870]\n",
    "\n",
    "b = [990, 561, 533, 517, 1019, 561, 1026, 568, 541, 978, 552, 567, 1008, 428, 1013, 525, 513, 516, 483, 1448, 539, 530, 432, 946, 1441, 499, 2623, 980, 1000, 488, 620, 507, 947, 505, 620, 488, 1000, 980, 2623, 499]\n",
    "# x_labels =  ['1a', '1b', '1c', '1d', '2a', '2b', '2c', '2d', '3a', '3b', '3c', '4a',\n",
    "#              '4b', '4c', '5a', '5b', '5c', '6a', '6b', '6c', '6d', '6e', '6f', '7a', \n",
    "#              '7b', '7c', '8a', '8b', '8c', '8d', '9a', '9b', '9c', '9d', '10a', '10b', \n",
    "#              '10c', '11a', '11b', '11c', '11d', '12a', '12b', '12c', '13a', '13b', '13c', \n",
    "#              '13d', '14a', '14b', '14c', '15a', '15b', '15c', '15d', '16a', '16b', '16c',\n",
    "#              '16d', '17a', '17b', '17c', '17d', '17e', '17f', '18a', '18b', '18c', '19a',\n",
    "#              '19b', '19c', '19d', '20a', '20b', '20c', '21a', '21b', '21c', '22a', '22b',\n",
    "#              '22c', '22d', '23a', '23b', '23c', '24a', '24b', '25a', '25b', '25c', '26a', \n",
    "#              '26b', '26c', '27a', '27b', '27c', '28a', '28b', '28c']\n",
    "print(len(a))\n",
    "sum = 0\n",
    "sum1 = 0\n",
    "sum2 = 0\n",
    "sum3 = 0\n",
    "x = []\n",
    "for i in range(0, len(a)):\n",
    "    if(a[i] > b[i]):\n",
    "        x.append(x_labels[i])\n",
    "        sum += 1\n",
    "    sum1 += a[i]\n",
    "    sum2 += b[i]\n",
    "    sum3 += 0 if b[i] == b[-1] else b[i]\n",
    "print(sum)\n",
    "print(sum1)\n",
    "print(sum2)\n",
    "print(sum3)\n",
    "print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.rand((3,20)) # 720\n",
    "b = a.view(3,4,5) # 40 18 \n",
    "c = a.view(3,5,4).transpose(1,2)\n",
    "print(a)\n",
    "print(c)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from leon_pl import *\n",
    "sqls_chunk = load_sql(['8c'])\n",
    "query_latency2, json_dict = getPG_latency(sqls_chunk[0], ENABLE_LEON=True, timeout_limit=0)\n",
    "node = postgres.ParsePostgresPlanJson(json_dict)\n",
    "print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hint_node = plans_lib.FilterScansOrJoins(node.Copy())\n",
    "hint_node.info['latency'], json = getPG_latency(sqls_chunk[0], hint_node.hint_str(), ENABLE_LEON=False, timeout_limit=0) # timeout 10s\n",
    "node1 = postgres.ParsePostgresPlanJson(json)\n",
    "print(node1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query_latency2)\n",
    "print(hint_node.info['latency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cali_all = torch.tensor([1.111, 1100])\n",
    "cali_all = random_tensor = 10 * torch.rand(cali_all.shape[0])\n",
    "def format_scientific_notation(number):\n",
    "            str_number = \"{:e}\".format(number)\n",
    "            mantissa, exponent = str_number.split('e')\n",
    "            mantissa = 9.994 if float(mantissa) >= 9.995 else float(mantissa)\n",
    "            mantissa = format(mantissa, '.2f')\n",
    "            exponent = int(exponent)\n",
    "            exponent = max(-9, min(9, exponent))\n",
    "            result = \"{},{},{:d}\".format(mantissa, '1' if exponent >= 0 else '0', abs(exponent))\n",
    "            return result\n",
    "cali_str = [format_scientific_notation(i) for i in cali_all.tolist()]\n",
    "print(cali_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# 示例用法\n",
    "import os\n",
    "\n",
    "folder_path = 'job'  # 请根据实际文件夹路径进行设置\n",
    "a =[]\n",
    "# 获取文件夹中的所有文件\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "# 遍历每个文件并处理\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # 判断是否为文件\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            # 读取文件内容\n",
    "            content = file.read()\n",
    "            from_where_content = re.search('FROM(.*)WHERE', content.replace(\"\\n\", \"\")).group(1)\n",
    "\n",
    "            # 提取别名\n",
    "            aliases = re.findall(r'AS (\\w+)', from_where_content)\n",
    "\n",
    "            aliases = \",\".join(sorted(aliases))\n",
    "            a.append(aliases)\n",
    "\n",
    "print(a)  # 输出: ['ct', 'it', 'mc', 'mi_idx', 't']\n",
    "\n",
    "print(\"处理完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./log/exp_v4.pkl', 'rb') as f:\n",
    "    exp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = None\n",
    "sum = 0\n",
    "for plan in exp['chn,ci,cn,ct,mc,rt,t']:\n",
    "    node = plan[0]\n",
    "    if node.info['sql_str'] != sql:\n",
    "        sql = node.info['sql_str']\n",
    "        sum += 1\n",
    "    print(sum)\n",
    "    print(node.info['latency'])\n",
    "    print(node.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from pympler import asizeof\n",
    "import gc\n",
    "from enum import Enum\n",
    "i = 0\n",
    "while True:\n",
    "    print(i)\n",
    "    i += 1\n",
    "    with open('./log/exp_v4.pkl', 'rb') as f:\n",
    "        exp = pickle.load(f)\n",
    "    del exp\n",
    "    gc.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureType(Enum):\n",
    "    numeric = \"numeric\"\n",
    "    categorical = \"categorical\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_size = asizeof.asizeof(exp['an,chn,ci,cn,it,mc,mi,n,rt,t'])\n",
    "\n",
    "# 将字节数转换为 MB\n",
    "total_size_in_mb = total_size / (1024.0 ** 2)\n",
    "\n",
    "# 打印结果\n",
    "print(f\"The total size of my_dict and its references is approximately: {total_size_in_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_elements = [sublist[1] for sublist in exp['an,chn,ci,cn,it,mc,mi,n,rt,t']]\n",
    "first_elements[0].shape\n",
    "memory_size_bytes = first_elements[0].element_size() * first_elements[0].numel()\n",
    "\n",
    "# 将字节数转换为 MB\n",
    "memory_size_mb = memory_size_bytes / (1024.0 ** 2)\n",
    "\n",
    "# 打印结果\n",
    "print(f\"The memory size of the tensor is approximately: {memory_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler import muppy, summary\n",
    "all_objects = muppy.get_objects()\n",
    "sum1 = summary.summarize(all_objects)# Prints out a summary of the large objects\n",
    "summary.print_(sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "def get_statistic(exp):\n",
    "    plans = exp\n",
    "\n",
    "    # get statistics\n",
    "    db_plans = {}\n",
    "    db_max_node_len = 0\n",
    "    runtimes = []\n",
    "    cards = []\n",
    "    costs = []\n",
    "\n",
    "    # get all node types, including subplan, implement by dfs\n",
    "    node_types = set()\n",
    "    for eq in exp.keys():\n",
    "        for plan in exp[eq]:\n",
    "            plan = plan[0]\n",
    "            try:\n",
    "                runtimes.append(plan.info['latency'])\n",
    "            except:\n",
    "                pass\n",
    "            costs.append(plan.cost)\n",
    "            cards.append(0)\n",
    "            node_types.add(plan.node_type)\n",
    "            stack = [plan]\n",
    "            node_len = 1\n",
    "            while len(stack) > 0:\n",
    "                node = stack.pop()\n",
    "                for child in node.children:\n",
    "                    node_len += 1\n",
    "                    node_types.add(child.node_type)\n",
    "                    stack.append(child)\n",
    "                    try:\n",
    "                        runtimes.append(child.info['latency'])\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    costs.append(child.cost)\n",
    "                    cards.append(0)\n",
    "            db_max_node_len = max(db_max_node_len, node_len)\n",
    "    print(db_max_node_len)\n",
    "\n",
    "    runtimes, cards, costs = np.array(runtimes), np.array(cards), np.array(costs)\n",
    "\n",
    "    # write to a json file\n",
    "    node_types = list(node_types)\n",
    "    statistics = {\n",
    "        \"Actual Total Time\": {\n",
    "            \"type\": str(FeatureType.numeric),\n",
    "            \"max\": float(np.max(runtimes)),\n",
    "            \"min\": float(np.min(runtimes)),\n",
    "            \"center\": float(np.median(runtimes)),\n",
    "            \"scale\": float(np.quantile(runtimes, 0.75))\n",
    "            - float(np.quantile(runtimes, 0.25)),\n",
    "        },\n",
    "        \"Plan Rows\": {\n",
    "            \"type\": str(FeatureType.numeric),\n",
    "            \"max\": float(np.max(cards)),\n",
    "            \"min\": float(np.min(cards)),\n",
    "            \"center\": float(np.median(cards)),\n",
    "            \"scale\": float(np.quantile(cards, 0.75)) - float(np.quantile(cards, 0.25)),\n",
    "        },\n",
    "        \"Total Cost\": {\n",
    "            \"type\": str(FeatureType.numeric),\n",
    "            \"max\": float(np.max(costs)),\n",
    "            \"min\": float(np.min(costs)),\n",
    "            \"center\": float(np.median(costs)),\n",
    "            \"scale\": float(np.quantile(costs, 0.75)) - float(np.quantile(costs, 0.25)),\n",
    "        },\n",
    "        \"node_types\": {\n",
    "            \"type\": str(FeatureType.categorical),\n",
    "            \"value_dict\": {node_type: i for i, node_type in enumerate(node_types)},\n",
    "        },\n",
    "    }\n",
    "    return statistics\n",
    "\n",
    "statistics = get_statistic(exp)\n",
    "with open(os.getcwd() + '/statistics1.json', \"w\") as f:\n",
    "    json.dump(statistics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_id='1,2,3,4,5,6'\n",
    "len(join_id.split(','))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设输入张量 x 的形状为 512 * 40 * node_length\n",
    "x = torch.rand((512, 40, 18))\n",
    "\n",
    "# 将 x 展平成二维张量\n",
    "# x = x.view(-1, 18)\n",
    "\n",
    "# 定义 nn.Sequential\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(18, 32),\n",
    "    nn.ReLU(),  # 假设你的激活函数是 ReLU\n",
    "    nn.Linear(32, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32)\n",
    ")\n",
    "\n",
    "# 将 x_flattened 输入到 MLP 中\n",
    "output = mlp(x)\n",
    "\n",
    "# 输出的形状为 20480 * 32，即 (512 * 40) * 32\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class EqSetInfo:\n",
    "    first_latency: float = 90000.0\n",
    "    current_latency: float = 90000.0\n",
    "    opt_time: float = 0.0\n",
    "    query_ids: List[str] = field(default_factory=list)\n",
    "\n",
    "# 示例数据\n",
    "all_set = [(1, EqSetInfo(query_ids=['1a', '2a'], opt_time=0.0)),\n",
    "           (2, EqSetInfo(query_ids=['3a', '4a'], opt_time=10.0)),\n",
    "           (3, EqSetInfo(query_ids=['5a', '6a'], opt_time=-5.0)),\n",
    "           (4, EqSetInfo(query_ids=['5a', '6a'], opt_time=0.0)),\n",
    "           (5, EqSetInfo(query_ids=['2a', '6a'], opt_time=5.0)),\n",
    "           (6, EqSetInfo(query_ids=['5a', '6a'], opt_time=0.0)),\n",
    "           (7, EqSetInfo(query_ids=['5a', '6a'], opt_time=-5.0)),\n",
    "           (8, EqSetInfo(query_ids=['5a', '6a'], opt_time=0.0))]\n",
    "\n",
    "sql_id = ['1a']\n",
    "\n",
    "# 过滤条件\n",
    "updated_set = [item for item in all_set if not (any(x in sql_id for x in item[1].query_ids) or item[1].opt_time==0)]\n",
    "\n",
    "# 打印结果\n",
    "for item in updated_set:\n",
    "    print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个1400x720的tensor，默认使用float32数据类型\n",
    "tensor_var = torch.zeros(1400, 720)\n",
    "\n",
    "# 获取tensor的数据类型\n",
    "dtype = tensor_var.dtype\n",
    "\n",
    "# 获取tensor的元素个数\n",
    "num_elements = tensor_var.numel()\n",
    "\n",
    "# 计算内存占用\n",
    "memory_size_bytes = num_elements * dtype.itemsize\n",
    "\n",
    "# 将字节数转换为兆字节\n",
    "memory_size_mb = memory_size_bytes / (1024 ** 2)\n",
    "\n",
    "print(f\"Tensor size: {memory_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def fix_json_msg(json):\n",
    "    # pattern_any = r'ANY \\((.*?):text\\[\\]\\)'\n",
    "    pattern_all = r'(ANY|ALL) \\((.*?):text\\[\\]\\)'\n",
    "\n",
    "    # matches_any = re.findall(pattern_any, json)\n",
    "    matches_all = re.findall(pattern_all, json)\n",
    "\n",
    "    # for match in matches_any:\n",
    "    #     extracted_string = match\n",
    "    #     cleaned_string = extracted_string.replace('\"', '')\n",
    "    #     json = json.replace(f'ANY ({extracted_string}):text[]', cleaned_string)\n",
    "    \n",
    "    for match_type, match in matches_all:\n",
    "        extracted_string = match\n",
    "        cleaned_string = extracted_string.replace('\"', '')\n",
    "        json = json.replace(extracted_string, cleaned_string)\n",
    "\n",
    "    return json\n",
    "\n",
    "json_data = '\"Base Restrict Info\": \"((mi.info)::text <> ALL (\\'{MET:,UK,Color,\\\"Black and White\\\"}\\'::text[]))\" '\n",
    "fixed_json = fix_json_msg(json_data)\n",
    "print(fixed_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_list_to_disk(file_path, data):\n",
    "    with open(file_path, 'ab') as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "def data_generator(file_path, batch_size=1000):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        try:\n",
    "            while True:\n",
    "                batch = []\n",
    "                for _ in range(batch_size):\n",
    "                    try:\n",
    "                        item = pickle.load(file)\n",
    "                        batch.append(item)\n",
    "                    except EOFError:\n",
    "                        break\n",
    "                \n",
    "                if not batch:\n",
    "                    break\n",
    "                \n",
    "                yield batch\n",
    "        except EOFError:\n",
    "            pass\n",
    "\n",
    "def process_batch(batch):\n",
    "    # 在这里对批次进行处理，这里简单地打印每个元素\n",
    "    print(batch)\n",
    "    # for item in batch:\n",
    "    #     print(item)\n",
    "\n",
    "# 生成示例数据\n",
    "large_list = list(range(999))\n",
    "for i in range(1000):\n",
    "    save_list_to_disk('large_list.pkl', [i])\n",
    "\n",
    "# 使用生成器逐批次读取和处理数据\n",
    "file_path = 'large_list.pkl'\n",
    "generator = data_generator(file_path, batch_size=100)\n",
    "print(generator)\n",
    "# for batch in generator:\n",
    "#     # print(len(batch[0]))\n",
    "#     process_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "# 创建 PyTorch 张量\n",
    "def fun():\n",
    "    a = torch.randn((90000, 1000, 27), device='cuda:3')\n",
    "\n",
    "# 删除张量\n",
    "# del a\n",
    "fun()\n",
    "# 清空 GPU 缓存\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_list = [torch.randint(0, 10, (y, 1)) for y in range(5, 8)]\n",
    "indexes_list[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/wyz/miniconda3/envs/leon/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-01-09 02:29:50,430\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from util import envs, plans_lib, treeconv, encoding\n",
    "with open('./log/exp_v4.pkl', 'rb') as f:\n",
    "    exp = pickle.load(f)\n",
    "# model = torch.load('./log/model.pth')\n",
    "# workload = envs.wordload_init('job')\n",
    "# nodeFeaturizer = plans_lib.TreeNodeFeaturizer_V2(workload.workload_info)\n",
    "# exp.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "sql 1\n",
      "index 33\n",
      "122176.241454\n",
      "785.783\n",
      "1\n",
      "sql 1\n",
      "index 42\n",
      "1206111.193127\n",
      "4212.074\n",
      "2\n",
      "sql 1\n",
      "index 36\n",
      "13379963288.678574\n",
      "1000000\n",
      "3\n",
      "sql 1\n",
      "index 34\n",
      "420566921.788489\n",
      "1000000\n",
      "4\n",
      "sql 1\n",
      "index 38\n",
      "1087877.618951\n",
      "1000000\n",
      "5\n",
      "sql 1\n",
      "index 44\n",
      "1086644.886451\n",
      "2354.253\n",
      "6\n",
      "sql 1\n",
      "index 40\n",
      "1086507.918951\n",
      "2423.367\n",
      "7\n",
      "sql 1\n",
      "index 45\n",
      "544548.112086\n",
      "2777.023\n",
      "8\n",
      "sql 1\n",
      "index 43\n",
      "409903.833577\n",
      "1000000\n",
      "9\n",
      "sql 1\n",
      "index 37\n",
      "183458.380908\n",
      "2317.86\n",
      "10\n",
      "sql 1\n",
      "index 35\n",
      "122097.271756\n",
      "1008.795\n",
      "11\n",
      "sql 1\n",
      "index 41\n",
      "308782.519062\n",
      "1000000\n",
      "12\n",
      "sql 1\n",
      "index 39\n",
      "237058.045297\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "num = 0\n",
    "dict1 = dict()\n",
    "# for i, plan in enumerate(exp['k,lt,mk,ml,t1,t2']): # 32b\n",
    "# for i, plan in enumerate(exp['an1,ci,cn,mc,n1,rt,t']): # 8d\n",
    "for i, plan in enumerate(exp['an,chn,ci,cn,it,mc,mi,n,rt,t']): # 10c\n",
    "\n",
    "    node = plan[0]\n",
    "    if node.info['sql_str'] not in dict1.keys():\n",
    "        num += 1\n",
    "        dict1[node.info['sql_str']] = num\n",
    "    # if dict1[node.info['sql_str']] == 2:\n",
    "    print(i)\n",
    "    # print(node.info['sql_str'])\n",
    "    print(\"sql\", dict1[node.info['sql_str']])\n",
    "    print(\"index\", node.info['index'])\n",
    "    print(node.cost)\n",
    "    hint_node = plans_lib.FilterScansOrJoins(node.Copy())\n",
    "    # print(hint_node.hint_str())\n",
    "    # print(node)\n",
    "    # print(node.info['query_feature'])\n",
    "    print(node.info['latency'])\n",
    "        ####\n",
    "        # model.eval()\n",
    "        # null_node = plans_lib.Binarize(node)\n",
    "        # trees, indexes = encoding.TreeConvFeaturize(nodeFeaturizer, [null_node], 200)\n",
    "        # cali = torch.tanh(model(node.info['query_feature'].unsqueeze(0), trees, indexes)).add(1).squeeze(1)\n",
    "        # print(cali)\n",
    "        # print(cali * torch.log(torch.tensor([node.cost])))\n",
    "\n",
    "# for j,i in enumerate(dict1.keys()):\n",
    "#     print(j+1, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5080.347275\n",
      "1 5080.314775\n",
      "2 5079.609497\n",
      "3 5080.319775\n",
      "4 199003.346644\n",
      "5 199002.641366\n",
      "6 199003.351644\n",
      "7 199003.364144\n",
      "8 199014.516644\n",
      "9 5080.3174\n",
      "10 5080.347275\n",
      "11 5080.314775\n",
      "12 5080.319775\n",
      "13 5091.482275\n",
      "14 5091.487275\n",
      "15 5091.499775\n",
      "16 199014.519144\n",
      "17 5080.328525\n",
      "18 5080.344775\n",
      "19 5080.312275\n",
      "20 5078.06981\n",
      "21 5080.317275\n",
      "22 1112142.885839\n",
      "23 1112140.643374\n",
      "24 1112142.890839\n",
      "25 1112142.903339\n",
      "26 1112154.595839\n",
      "27 5080.3149\n",
      "28 5080.344775\n",
      "29 5080.312275\n",
      "30 5080.317275\n",
      "31 5092.019775\n",
      "32 5092.024775\n",
      "33 5092.037275\n",
      "34 1112154.598339\n",
      "35 5080.326025\n",
      "36 5080.344775\n",
      "37 5080.312275\n",
      "38 5078.069839\n",
      "39 5080.317275\n",
      "40 68595.999112\n",
      "41 68593.756676\n",
      "42 68596.004112\n",
      "43 68596.016612\n",
      "44 68607.709112\n",
      "45 5080.3149\n",
      "46 5080.344775\n",
      "47 5080.312275\n",
      "48 5080.317275\n",
      "49 5092.019775\n",
      "50 5092.024775\n",
      "51 5092.037275\n",
      "52 68607.711612\n",
      "53 5080.326025\n",
      "54 31888.114087\n",
      "55 31888.091587\n",
      "56 29262.396865\n",
      "57 31888.101587\n",
      "58 282197.203131\n",
      "59 279571.50841\n",
      "60 282197.213131\n",
      "61 282197.210631\n",
      "62 284385.861192\n",
      "63 31888.094212\n",
      "64 31888.114087\n",
      "65 61150.033173\n",
      "66 31888.099087\n",
      "67 63338.698734\n",
      "68 34076.764647\n",
      "69 34076.764647\n",
      "70 284385.863692\n",
      "71 31888.096587\n",
      "72 34302.775257\n",
      "73 1965570.752063\n",
      "74 34301.388117\n",
      "75 2192822.890294\n",
      "76 261553.526348\n",
      "77 261552.713848\n",
      "78 284165.556662\n",
      "79 34300.654367\n",
      "80 34302.775257\n",
      "81 34301.218117\n",
      "82 34301.553117\n",
      "83 56914.07343\n",
      "84 56914.40843\n",
      "85 56915.61557\n",
      "86 284165.721662\n",
      "87 56914.08718\n",
      "88 34301.231867\n",
      "89 91644.865246\n",
      "90 162766.788445\n",
      "91 91644.69723\n",
      "92 203550.775626\n",
      "93 132428.68441\n",
      "94 132428.52191\n",
      "95 158814.851911\n",
      "96 91644.54848\n",
      "97 91644.865246\n",
      "98 91644.65723\n",
      "99 91644.73223\n",
      "100 118030.99973\n",
      "101 118031.07473\n",
      "102 118031.192747\n",
      "103 158814.886911\n",
      "104 91644.67098\n",
      "105 31500.679629\n",
      "106 16923766.898079\n",
      "107 31339.239312\n",
      "108 16946379.753392\n",
      "109 53952.094626\n",
      "110 53910.519626\n",
      "111 140441.030243\n",
      "112 53914.674626\n",
      "113 31301.819312\n",
      "114 31500.679629\n",
      "115 31330.916812\n",
      "116 31347.556812\n",
      "117 117861.43993\n",
      "118 117878.07993\n",
      "119 118031.187747\n",
      "120 140449.347743\n",
      "121 31330.930562\n",
      "122 500905.752849\n",
      "123 30849680.805421\n",
      "124 500590.414549\n",
      "125 31485289.474459\n",
      "126 1136199.083587\n",
      "127 1135746.081682\n",
      "128 1162135.394285\n",
      "129 1136131.828587\n",
      "130 500523.159549\n",
      "131 500905.752849\n",
      "132 500575.462049\n",
      "133 500605.362049\n",
      "134 526964.787152\n",
      "135 526994.687152\n",
      "136 527295.062952\n",
      "137 1162150.336785\n",
      "138 500575.545799\n",
      "139 43652.298567\n",
      "140 43651.42692\n",
      "141 43651.64192\n",
      "142 130184.192502\n",
      "143 130184.407502\n",
      "144 130185.049149\n",
      "145 152808.937815\n",
      "146 43651.44067\n",
      "147 43652.298567\n",
      "148 256718.186553\n",
      "149 43651.53692\n",
      "150 279343.241866\n",
      "151 66276.592233\n",
      "152 66276.079733\n",
      "153 152808.832815\n",
      "154 66276.128483\n",
      "155 43651.07317\n",
      "{'QueryId': '43651.06067', 'Plan': {'Node Type': 'HashJoin', 'Node Type ID': '181', 'Relation IDs': 'k mk t mi_idx it2 mi it1 kt', 'Join Cond': 'it1.id = mi.info_type_id, it2.id = mi_idx.info_type_id, k.id = mk.keyword_id, kt.id = t.kind_id, mi.movie_id = mi_idx.movie_id, mk.movie_id = mi.movie_id, mk.movie_id = mi_idx.movie_id, t.id = mi.movie_id, t.id = mi_idx.movie_id, t.id = mk.movie_id', 'Path Target': 't.title, mi_idx.info', 'Startup Cost': 5075.890586, 'Total Cost': 43651.07317, 'Plan Rows': 1.0, 'Plan Width': 23, 'Plans': [{'Node Type': 'NestLoop', 'Node Type ID': '179', 'Relation IDs': 'mi_idx it2 mi it1', 'Join Cond': 'it1.id = mi.info_type_id, it2.id = mi_idx.info_type_id, mi.movie_id = mi_idx.movie_id', 'Path Target': 'mi_idx.info, mi_idx.movie_id, mi.movie_id', 'Startup Cost': 2.86, 'Total Cost': 38577.871333, 'Plan Rows': 43.0, 'Plan Width': 14, 'Plans': [{'Node Type': 'SeqScan', 'Node Type ID': '169', 'Relation IDs': 'it1', 'Base Restrict Info': \"((it1.info)::text = 'countries'::text)\", 'Path Target': 'it1.id', 'Startup Cost': 0.0, 'Total Cost': 2.4125, 'Plan Rows': 1.0, 'Plan Width': 4}, {'Node Type': 'NestLoop', 'Node Type ID': '179', 'Relation IDs': 'mi_idx it2 mi', 'Join Cond': 'it2.id = mi_idx.info_type_id, mi.movie_id = mi_idx.movie_id', 'Path Target': 'mi_idx.info, mi_idx.movie_id, mi.info_type_id, mi.movie_id', 'Startup Cost': 2.86, 'Total Cost': 38514.471333, 'Plan Rows': 4879.0, 'Plan Width': 18, 'Plans': [{'Node Type': 'HashJoin', 'Node Type ID': '181', 'Relation IDs': 'mi_idx it2', 'Join Cond': 'it2.id = mi_idx.info_type_id', 'Path Target': 'mi_idx.info, mi_idx.movie_id', 'Startup Cost': 2.425, 'Total Cost': 26211.939, 'Plan Rows': 3328.0, 'Plan Width': 10, 'Plans': [{'Node Type': 'SeqScan', 'Node Type ID': '169', 'Relation IDs': 'mi_idx', 'Base Restrict Info': \"((mi_idx.info)::text > '6.0'::text)\", 'Path Target': 'mi_idx.info, mi_idx.info_type_id, mi_idx.movie_id', 'Startup Cost': 0.0, 'Total Cost': 25185.4375, 'Plan Rows': 376020.0, 'Plan Width': 14}, {'Node Type': 'SeqScan', 'Node Type ID': '169', 'Relation IDs': 'it2', 'Base Restrict Info': \"((it2.info)::text = 'rating'::text)\", 'Path Target': 'it2.id', 'Startup Cost': 0.0, 'Total Cost': 2.4125, 'Plan Rows': 1.0, 'Plan Width': 4}]}, {'Node Type': 'IndexScan', 'Node Type ID': '170', 'Relation IDs': 'mi', 'Base Restrict Info': \"((mi.info)::text = ANY ('{Sweden,Norway,Germany,Denmark,Swedish,Denish,Norwegian,German,USA,American}'::text[]))\", 'Required Outer': 'mi_idx', 'Path Target': 'mi.info_type_id, mi.movie_id', 'Startup Cost': 0.435, 'Total Cost': 3.676674, 'Plan Rows': 2.0, 'Plan Width': 8}]}]}, {'Node Type': 'NestLoop', 'Node Type ID': '179', 'Relation IDs': 'k mk t kt', 'Join Cond': 'k.id = mk.keyword_id, kt.id = t.kind_id, t.id = mk.movie_id', 'Path Target': 'mk.movie_id, t.title, t.id', 'Startup Cost': 0.8625, 'Total Cost': 5073.018086, 'Plan Rows': 1.0, 'Plan Width': 25, 'Plans': [{'Node Type': 'NestLoop', 'Node Type ID': '179', 'Relation IDs': 'k mk t', 'Join Cond': 'k.id = mk.keyword_id, t.id = mk.movie_id', 'Path Target': 'mk.movie_id, t.title, t.kind_id, t.id', 'Startup Cost': 0.8625, 'Total Cost': 5071.918086, 'Plan Rows': 1.0, 'Plan Width': 29, 'Plans': [{'Node Type': 'NestLoop', 'Node Type ID': '179', 'Relation IDs': 'k mk', 'Join Cond': 'k.id = mk.keyword_id', 'Path Target': 'mk.movie_id', 'Startup Cost': 0.4325, 'Total Cost': 5038.721012, 'Plan Rows': 67.0, 'Plan Width': 4, 'Plans': [{'Node Type': 'SeqScan', 'Node Type ID': '169', 'Relation IDs': 'k', 'Base Restrict Info': \"((k.keyword)::text = ANY ('{murder,murder-in-title}'::text[]))\", 'Path Target': 'k.id', 'Startup Cost': 0.0, 'Total Cost': 2626.125, 'Plan Rows': 2.0, 'Plan Width': 4}, {'Node Type': 'IndexScan', 'Node Type ID': '170', 'Relation IDs': 'mk', 'Required Outer': 'k', 'Path Target': 'mk.keyword_id, mk.movie_id', 'Startup Cost': 0.4325, 'Total Cost': 1203.278006, 'Plan Rows': 302.0, 'Plan Width': 8}]}, {'Node Type': 'IndexScan', 'Node Type ID': '170', 'Relation IDs': 't', 'Base Restrict Info': \"(t.production_year > 2010), (((t.title)::text ~~ '%murder%'::text) OR ((t.title)::text ~~ '%Murder%'::text) OR ((t.title)::text ~~ '%Mord%'::text))\", 'Required Outer': 'mk', 'Path Target': 't.title, t.kind_id, t.id', 'Startup Cost': 0.43, 'Total Cost': 0.495461, 'Plan Rows': 1.0, 'Plan Width': 25}]}, {'Node Type': 'SeqScan', 'Node Type ID': '169', 'Relation IDs': 'kt', 'Base Restrict Info': \"((kt.kind)::text = 'movie'::text)\", 'Path Target': 'kt.id', 'Startup Cost': 0.0, 'Total Cost': 1.0875, 'Plan Rows': 1.0, 'Plan Width': 4}]}]}}\n",
      "156 94294.940931\n",
      "157 94294.908431\n",
      "158 94294.913431\n",
      "159 120683.493396\n",
      "160 120683.498396\n",
      "161 120683.510896\n",
      "162 161467.498077\n",
      "163 94294.922181\n",
      "164 94294.940931\n",
      "165 94294.908431\n",
      "166 94294.913431\n",
      "167 135078.908112\n",
      "168 135078.913112\n",
      "169 135078.925612\n",
      "170 161467.498077\n",
      "171 94294.922181\n",
      "172 34428.018066\n",
      "173 34427.985566\n",
      "174 34427.990566\n",
      "175 261680.461298\n",
      "176 261680.466298\n",
      "177 261680.478798\n",
      "178 284293.321611\n",
      "179 34427.999316\n",
      "180 34428.018066\n",
      "181 34427.985566\n",
      "182 34427.990566\n",
      "183 57040.84088\n",
      "184 57040.84588\n",
      "185 57040.85838\n",
      "186 284293.321611\n",
      "187 57040.85463\n",
      "188 34427.999316\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./error.pkl', 'rb') as f:\n",
    "    message = pickle.load(f)\n",
    "for j,i in enumerate(message):\n",
    "    # if float(i['Plan']['Total Cost']) == 84058.53:\n",
    "    #     print(\"yes\")\n",
    "    print(j,float(i['Plan']['Total Cost']))\n",
    "    if j== 155:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT mi.info,\n",
      "       mi_idx.info,\n",
      "       n.name,\n",
      "       t.title\n",
      "FROM info_type AS it2,\n",
      "     keyword AS k,\n",
      "     movie_keyword AS mk,\n",
      "     movie_info_idx AS mi_idx,\n",
      "     complete_cast AS cc,\n",
      "     comp_cast_type AS cct1,\n",
      "     comp_cast_type AS cct2,\n",
      "     cast_info AS ci,\n",
      "     movie_info AS mi,\n",
      "     info_type AS it1,\n",
      "     name AS n,\n",
      "     title AS t\n",
      "WHERE cct1.id = cc.subject_id\n",
      "  AND cct2.id = cc.status_id\n",
      "  AND ci.movie_id = cc.movie_id\n",
      "  AND ci.movie_id = mi.movie_id\n",
      "  AND ci.movie_id = mi_idx.movie_id\n",
      "  AND ci.movie_id = mk.movie_id\n",
      "  AND it1.id = mi.info_type_id\n",
      "  AND it2.id = mi_idx.info_type_id\n",
      "  AND k.id = mk.keyword_id\n",
      "  AND mi.movie_id = cc.movie_id\n",
      "  AND mi.movie_id = mi_idx.movie_id\n",
      "  AND mi.movie_id = mk.movie_id\n",
      "  AND mi_idx.movie_id = cc.movie_id\n",
      "  AND mi_idx.movie_id = mk.movie_id\n",
      "  AND mk.movie_id = cc.movie_id\n",
      "  AND n.id = ci.person_id\n",
      "  AND t.id = cc.movie_id\n",
      "  AND t.id = ci.movie_id\n",
      "  AND t.id = mi.movie_id\n",
      "  AND t.id = mi_idx.movie_id\n",
      "  AND t.id = mk.movie_id\n",
      "  AND ((cct1.kind)::text = ANY ('{cast,crew}'::text[]))\n",
      "  AND ((cct2.kind)::text = 'complete+verified'::text)\n",
      "  AND ((ci.note)::text = ANY ('{(writer),(head writer),(written by),(story),(story editor)}'::text[]))\n",
      "  AND ((it1.info)::text = 'genres'::text)\n",
      "  AND ((it2.info)::text = 'votes'::text)\n",
      "  AND ((k.keyword)::text = ANY ('{murder,violence,blood,gore ,death,female-nudity,hospital}'::text[]))\n",
      "  AND ((mi.info)::text = ANY ('{Horror,Thriller}'::text[]))\n",
      "  AND ((n.gender)::text = 'm'::text)\n",
      "  AND (t.production_year > 2000);\n"
     ]
    }
   ],
   "source": [
    "import sqlparse\n",
    "sql = \"SELECT mi.info,mi_idx.info,n.name,t.title FROM   info_type AS it2, keyword AS k, movie_keyword AS mk, movie_info_idx AS mi_idx, complete_cast AS cc, comp_cast_type AS cct1, comp_cast_type AS cct2, cast_info AS ci, movie_info AS mi, info_type AS it1, name AS n, title AS t  WHERE cct1.id = cc.subject_id AND cct2.id = cc.status_id AND ci.movie_id = cc.movie_id AND ci.movie_id = mi.movie_id AND ci.movie_id = mi_idx.movie_id AND ci.movie_id = mk.movie_id AND it1.id = mi.info_type_id AND it2.id = mi_idx.info_type_id AND k.id = mk.keyword_id AND mi.movie_id = cc.movie_id AND mi.movie_id = mi_idx.movie_id AND mi.movie_id = mk.movie_id AND mi_idx.movie_id = cc.movie_id AND mi_idx.movie_id = mk.movie_id AND mk.movie_id = cc.movie_id AND n.id = ci.person_id AND t.id = cc.movie_id AND t.id = ci.movie_id AND t.id = mi.movie_id AND t.id = mi_idx.movie_id AND t.id = mk.movie_id AND ((cct1.kind)::text = ANY ('{cast,crew}'::text[])) AND ((cct2.kind)::text = 'complete+verified'::text) AND ((ci.note)::text = ANY ('{(writer),(head writer),(written by),(story),(story editor)}'::text[])) AND ((it1.info)::text = 'genres'::text) AND ((it2.info)::text = 'votes'::text) AND ((k.keyword)::text = ANY ('{murder,violence,blood,gore ,death,female-nudity,hospital}'::text[])) AND ((mi.info)::text = ANY ('{Horror,Thriller}'::text[])) AND ((n.gender)::text = 'm'::text) AND (t.production_year > 2000);\"\n",
    "sql = sqlparse.format(sql, reindent=True, keyword_case=\"upper\")\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT mi.info,\n",
      "       mi_idx.info,\n",
      "       n.name,\n",
      "       t.title\n",
      "FROM comp_cast_type AS cct2,\n",
      "     info_type AS it2,\n",
      "     keyword AS k,\n",
      "     movie_keyword AS mk,\n",
      "     movie_info_idx AS mi_idx,\n",
      "     complete_cast AS cc,\n",
      "     comp_cast_type AS cct1,\n",
      "     cast_info AS ci,\n",
      "     movie_info AS mi,\n",
      "     info_type AS it1,\n",
      "     name AS n,\n",
      "     title AS t\n",
      "WHERE cct1.id = cc.subject_id\n",
      "  AND cct2.id = cc.status_id\n",
      "  AND ci.movie_id = cc.movie_id\n",
      "  AND ci.movie_id = mi.movie_id\n",
      "  AND ci.movie_id = mi_idx.movie_id\n",
      "  AND ci.movie_id = mk.movie_id\n",
      "  AND it1.id = mi.info_type_id\n",
      "  AND it2.id = mi_idx.info_type_id\n",
      "  AND k.id = mk.keyword_id\n",
      "  AND mi.movie_id = cc.movie_id\n",
      "  AND mi.movie_id = mi_idx.movie_id\n",
      "  AND mi.movie_id = mk.movie_id\n",
      "  AND mi_idx.movie_id = cc.movie_id\n",
      "  AND mi_idx.movie_id = mk.movie_id\n",
      "  AND mk.movie_id = cc.movie_id\n",
      "  AND n.id = ci.person_id\n",
      "  AND t.id = cc.movie_id\n",
      "  AND t.id = ci.movie_id\n",
      "  AND t.id = mi.movie_id\n",
      "  AND t.id = mi_idx.movie_id\n",
      "  AND t.id = mk.movie_id\n",
      "  AND ((cct1.kind)::text = ANY ('{cast,crew}'::text[]))\n",
      "  AND ((cct2.kind)::text = 'complete+verified'::text)\n",
      "  AND ((ci.note)::text = ANY ('{(writer),(head writer),(written by),(story),(story editor)}'::text[]))\n",
      "  AND ((it1.info)::text = 'genres'::text)\n",
      "  AND ((it2.info)::text = 'votes'::text)\n",
      "  AND ((k.keyword)::text = ANY ('{murder,violence,blood,gore,death,female-nudity,hospital}'::text[]))\n",
      "  AND ((mi.info)::text = ANY ('{Horror,Thriller}'::text[]))\n",
      "  AND ((n.gender)::text = 'm'::text)\n",
      "  AND (t.production_year > 2000);\n"
     ]
    }
   ],
   "source": [
    "import sqlparse\n",
    "sql = \"SELECT mi.info,mi_idx.info,n.name,t.title   FROM   comp_cast_type AS cct2, info_type AS it2, keyword AS k, movie_keyword AS mk, movie_info_idx AS mi_idx, complete_cast AS cc, comp_cast_type AS cct1, cast_info AS ci, movie_info AS mi, info_type AS it1, name AS n, title AS t  WHERE cct1.id = cc.subject_id AND cct2.id = cc.status_id AND ci.movie_id = cc.movie_id AND ci.movie_id = mi.movie_id AND ci.movie_id = mi_idx.movie_id AND ci.movie_id = mk.movie_id AND it1.id = mi.info_type_id AND it2.id = mi_idx.info_type_id AND k.id = mk.keyword_id AND mi.movie_id = cc.movie_id AND mi.movie_id = mi_idx.movie_id AND mi.movie_id = mk.movie_id AND mi_idx.movie_id = cc.movie_id AND mi_idx.movie_id = mk.movie_id AND mk.movie_id = cc.movie_id AND n.id = ci.person_id AND t.id = cc.movie_id AND t.id = ci.movie_id AND t.id = mi.movie_id AND t.id = mi_idx.movie_id AND t.id = mk.movie_id AND ((cct1.kind)::text = ANY ('{cast,crew}'::text[])) AND ((cct2.kind)::text = 'complete+verified'::text) AND ((ci.note)::text = ANY ('{(writer),(head writer),(written by),(story),(story editor)}'::text[])) AND ((it1.info)::text = 'genres'::text) AND ((it2.info)::text = 'votes'::text) AND ((k.keyword)::text = ANY ('{murder,violence,blood,gore,death,female-nudity,hospital}'::text[])) AND ((mi.info)::text = ANY ('{Horror,Thriller}'::text[])) AND ((n.gender)::text = 'm'::text) AND (t.production_year > 2000);\"\n",
    "sql = sqlparse.format(sql, reindent=True, keyword_case=\"upper\")\n",
    "print(sql)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
