{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "import lightning.pytorch.callbacks as plc\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import pytorch_lightning.loggers as pl_loggers\n",
    "import pickle\n",
    "import math\n",
    "import json\n",
    "from test_case import configs\n",
    "import numpy as np\n",
    "import wandb\n",
    "import random\n",
    "DEVICE = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqFormer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dim,\n",
    "        output_dim,\n",
    "        mlp_activation=\"ReLU\",\n",
    "        transformer_activation=\"gelu\",\n",
    "        mlp_dropout=0.1,\n",
    "        transformer_dropout=0.1,\n",
    "    ):\n",
    "        super(SeqFormer, self).__init__()\n",
    "        # input_dim: node bits\n",
    "        self.tranformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=input_dim,\n",
    "                dim_feedforward=hidden_dim,\n",
    "                nhead=1,\n",
    "                batch_first=True,\n",
    "                activation=transformer_activation,\n",
    "                dropout=transformer_dropout,\n",
    "            ),\n",
    "            num_layers=1,\n",
    "        )\n",
    "        self.node_length = input_dim\n",
    "        if mlp_activation == \"ReLU\":\n",
    "            self.mlp_activation = nn.ReLU()\n",
    "        elif mlp_activation == \"GELU\":\n",
    "            self.mlp_activation = nn.GELU()\n",
    "        elif mlp_activation == \"LeakyReLU\":\n",
    "            self.mlp_activation = nn.LeakyReLU()\n",
    "        # self.mlp_hidden_dims = [128, 64, 32]\n",
    "        self.mlp_hidden_dims = [256, 128, 1]\n",
    "        self.mlp = nn.Sequential(\n",
    "            *[\n",
    "                nn.Linear(self.node_length, self.mlp_hidden_dims[0]),\n",
    "                nn.Dropout(mlp_dropout),\n",
    "                self.mlp_activation,\n",
    "                nn.Linear(self.mlp_hidden_dims[0], self.mlp_hidden_dims[1]),\n",
    "                nn.Dropout(mlp_dropout),\n",
    "                self.mlp_activation,\n",
    "                nn.Linear(self.mlp_hidden_dims[1], output_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.apply(self._init_weights)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # change x shape to (batch, seq_len, input_size) from (batch, len)\n",
    "        # one node is 18 bits\n",
    "        # x = x.view(x.shape[0], self.node_length, -1)\n",
    "        # x = x.transpose(1,2)\n",
    "        x = x.view(x.shape[0], -1, self.node_length)\n",
    "        # attn_mask = attn_mask.repeat(4,1,1)\n",
    "        out = self.tranformer_encoder(x, mask=attn_mask)\n",
    "        # out = self.transformer_decoder(out, out, tgt_mask=attn_mask)\n",
    "        out = self.mlp(out)\n",
    "        # out = (torch.tanh(out).squeeze(dim=2) * 5).add(5) \n",
    "        # out = torch.tanh(out).squeeze(dim=2).add(1) * 5\n",
    "        out = torch.tanh(out).squeeze(dim=2).add(1)\n",
    "        return out # [0, 1] -> [1, 2] [??]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/wyz/miniconda3/envs/leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "Transformer_model = SeqFormer(\n",
    "                        input_dim=configs['node_length'],\n",
    "                        hidden_dim=256,\n",
    "                        output_dim=1,\n",
    "                        mlp_activation=\"ReLU\",\n",
    "                        transformer_activation=\"gelu\",\n",
    "                        mlp_dropout=0.1,\n",
    "                        transformer_dropout=0.1,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PL_Leon(pl.LightningModule):\n",
    "    def __init__(self, model, optimizer_state_dict=None, learning_rate=0.001):\n",
    "        super(PL_Leon, self).__init__()\n",
    "        self.model = model\n",
    "        self.optimizer_state_dict = optimizer_state_dict\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "    def forward(self, batch_pairs):\n",
    "        pass\n",
    "\n",
    "    def getBatchPairsLoss(self, labels, costs1, costs2, encoded_plans1, encoded_plans2, attns1, attns2):\n",
    "        \"\"\"\n",
    "        batch_pairs: a batch of train pairs\n",
    "        return. a batch of loss\n",
    "        \"\"\"\n",
    "        loss_fn = nn.BCELoss(reduction='none')\n",
    "        batsize = costs1.shape[0]\n",
    "        encoded_plans = torch.cat((encoded_plans1, encoded_plans2), dim=0)\n",
    "        attns = torch.cat((attns1, attns2), dim=0)\n",
    "        cali = self.model(encoded_plans, attns) # cali.shape [# of plan, pad_length] cali 是归一化后的基数估计\n",
    "        cali = cali[:, 0]\n",
    "        costs = torch.cat((costs1, costs2), dim=0)\n",
    "        calied_cost = torch.log(costs) * cali\n",
    "        try:\n",
    "            sigmoid = F.sigmoid(-(calied_cost[:batsize] - calied_cost[batsize:]))\n",
    "            loss = loss_fn(sigmoid, labels.float())\n",
    "        except:\n",
    "            print(calied_cost, sigmoid)\n",
    "        with torch.no_grad():\n",
    "            prediction = torch.round(sigmoid)\n",
    "            accuracy = torch.sum(prediction == labels).item() / len(labels)\n",
    "        return loss, accuracy\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        labels, costs1, costs2, encoded_plans1, encoded_plans2, attns1, attns2, latency1, latency2 = batch\n",
    "        loss, acc  = self.getBatchPairsLoss(labels, costs1, costs2, encoded_plans1, encoded_plans2, attns1, attns2)\n",
    "        loss = (torch.abs(latency1 - latency2) * loss / 90000).mean()\n",
    "        self.log_dict({'t_loss': loss, 't_acc': acc}, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        labels, costs1, costs2, encoded_plans1, encoded_plans2, attns1, attns2, latency1, latency2 = batch\n",
    "        loss, acc  = self.getBatchPairsLoss(labels, costs1, costs2, encoded_plans1, encoded_plans2, attns1, attns2)\n",
    "        loss = (torch.abs(latency1 - latency2) * loss / 90000).mean()\n",
    "        self.log_dict({'v_loss': loss, 'v_acc': acc}, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.001)\n",
    "        if self.optimizer_state_dict is not None:\n",
    "            curr = optimizer.state_dict()['param_groups'][0]['params']\n",
    "            prev = self.optimizer_state_dict['param_groups'][0]['params']\n",
    "            assert curr == prev, (curr, prev)\n",
    "            optimizer.load_state_dict(self.optimizer_state_dict)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = self.learning_rate\n",
    "            assert optimizer.state_dict(\n",
    "            )['param_groups'][0]['lr'] == self.learning_rate\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(prev_optimizer_state_dict=None):\n",
    "    model = Transformer_model.to(DEVICE)\n",
    "    model = PL_Leon(model, prev_optimizer_state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeonDataset(Dataset):\n",
    "    def __init__(self, labels, costs1, costs2, encoded_plans1, encoded_plans2, attns1, attns2, latency1, latency2):\n",
    "        self.labels = labels\n",
    "        self.costs1 = costs1\n",
    "        self.costs2 = costs2\n",
    "        self.encoded_plans1 = encoded_plans1\n",
    "        self.encoded_plans2 = encoded_plans2\n",
    "        self.attns1 = attns1\n",
    "        self.attns2 = attns2\n",
    "        self.latency1 = latency1\n",
    "        self.latency2 = latency2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.labels[idx],\n",
    "                self.costs1[idx],\n",
    "                self.costs2[idx],\n",
    "                self.encoded_plans1[idx],\n",
    "                self.encoded_plans2[idx],\n",
    "                self.attns1[idx],\n",
    "                self.attns2[idx],\n",
    "                self.latency1[idx],\n",
    "                self.latency2[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(pairs):\n",
    "    labels = []\n",
    "    costs1 = []\n",
    "    costs2 = []\n",
    "    encoded_plans1 = []\n",
    "    encoded_plans2 = []\n",
    "    attns1 = []\n",
    "    attns2 = []\n",
    "    latency1 = []\n",
    "    latency2 = []\n",
    "    for pair in pairs:\n",
    "        if pair[0][0].info['latency'] > pair[1][0].info['latency']:\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "        labels.append(label)\n",
    "        costs1.append(pair[0][0].cost)\n",
    "        costs2.append(pair[1][0].cost)\n",
    "        encoded_plans1.append(pair[0][1])\n",
    "        encoded_plans2.append(pair[1][1])\n",
    "        attns1.append(pair[0][2])\n",
    "        attns2.append(pair[1][2])\n",
    "        latency1.append(pair[0][0].info['latency'])\n",
    "        latency2.append(pair[1][0].info['latency'])\n",
    "    labels = torch.tensor(labels)\n",
    "    costs1 = torch.tensor(costs1)\n",
    "    costs2 = torch.tensor(costs2)\n",
    "    encoded_plans1 = torch.stack(encoded_plans1)\n",
    "    encoded_plans2 = torch.stack(encoded_plans2)\n",
    "    attns1 = torch.stack(attns1)\n",
    "    attns2 = torch.stack(attns2)\n",
    "    latency1 = torch.tensor(latency1)\n",
    "    latency2 = torch.tensor(latency2)\n",
    "    dataset = LeonDataset(labels, costs1, costs2, encoded_plans1, encoded_plans2, attns1, attns2, latency1, latency2)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_callbacks(logger):\n",
    "    callbacks = []\n",
    "    callbacks.append(plc.EarlyStopping(\n",
    "        monitor='v_acc',\n",
    "        mode='max',\n",
    "        patience=5,\n",
    "        min_delta=0.001\n",
    "    ))\n",
    "    if logger:\n",
    "        callbacks.append(plc.ModelCheckpoint(\n",
    "            dirpath= logger.experiment.dir,\n",
    "            monitor='val_scan',\n",
    "            filename='best-{epoch:02d}-{val_scan:.3f}',\n",
    "            save_top_k=1,\n",
    "            mode='min',\n",
    "            save_last=False\n",
    "        ))\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Getpair(exp):\n",
    "    pairs = []\n",
    "    for eq in exp.keys():\n",
    "        for j in exp[eq]:\n",
    "            for k in exp[eq]:\n",
    "                if (j[0].info['sql_str'] == k[0].info['sql_str']) and (j[0].hint_str() == k[0].hint_str()): # sql 和 hint 都相同\n",
    "                    continue\n",
    "                # if (j[0].info['latency'] == k[0].info['latency']): # latency 相同 1s之内不把他train_pair\n",
    "                if max(j[0].info['latency'],k[0].info['latency']) / min(j[0].info['latency'],k[0].info['latency']) < 1.2:\n",
    "                    continue\n",
    "                # if j[0].info['latency'] == 90000 or k[0].info['latency'] == 90000:\n",
    "                #     continue\n",
    "                tem = [j, k]\n",
    "                pairs.append(tem)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../log/exp.pkl', 'rb') as f:\n",
    "        exp = pickle.load(f)\n",
    "current_directory = os.getcwd()\n",
    "# 获取上一级目录\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "logger =  pl_loggers.WandbLogger(save_dir=parent_directory + '/logs', name=\"test458\", project='leon3')\n",
    "prev_optimizer_state_dict = None\n",
    "model = load_model().to(DEVICE)\n",
    "callbacks = load_callbacks(logger=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "cn,k,mc,mk,t 458\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "filtered_keys = [key for key in exp.keys() if len(key) > 10 and len(exp[key]) < 500 and len(exp[key]) > 457]\n",
    "print(len(filtered_keys))\n",
    "for eq in filtered_keys:\n",
    "    print(eq, len(exp[eq]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_pairs) 1228676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "train_pairs = Getpair(exp)\n",
    "print(\"len(train_pairs)\" ,len(train_pairs))\n",
    "leon_dataset = prepare_dataset(train_pairs)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\",\n",
    "                        devices=[2],\n",
    "                        max_epochs=40,\n",
    "                        logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwyz12234\u001b[0m (\u001b[33mleon1\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data1/wyz/online/LEONForPostgres/logs/wandb/run-20231222_074341-tqdvc8al</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leon1/leon3/runs/tqdvc8al' target=\"_blank\">test458</a></strong> to <a href='https://wandb.ai/leon1/leon3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/leon1/leon3' target=\"_blank\">https://wandb.ai/leon1/leon3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/leon1/leon3/runs/tqdvc8al' target=\"_blank\">https://wandb.ai/leon1/leon3/runs/tqdvc8al</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | SeqFormer | 48.8 K\n",
      "------------------------------------\n",
      "48.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "48.8 K    Total params\n",
      "0.195     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/wyz/miniconda3/envs/leon/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/wyz/miniconda3/envs/leon/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  91%|█████████ | 2187/2400 [01:11<00:07, 30.40it/s, v_num=c8al]"
     ]
    }
   ],
   "source": [
    "dataloader_train = DataLoader(leon_dataset, batch_size=512, shuffle=True, num_workers=0)\n",
    "dataloader_val = DataLoader(leon_dataset, batch_size=512, shuffle=False, num_workers=0)\n",
    "trainer.fit(model, dataloader_train, dataloader_val)\n",
    "prev_optimizer_state_dict = trainer.optimizers[0].state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
