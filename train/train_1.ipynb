{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import pickle\n",
    "with open('../log/exp.pkl', 'rb') as f:\n",
    "    exp = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cn,ct,k,lt,mc,mi,mk,ml,t\n",
      "169\n",
      "torch.Size([40, 40])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keys = list(exp.keys())\n",
    "print(keys[0])\n",
    "print(len(exp[keys[0]]))\n",
    "print(exp[keys[0]][0][2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all eqset: 82\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n",
      "torch.Size([1, 720])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import pickle\n",
    "from leon_experience import Experience\n",
    "import random\n",
    "\n",
    "class BucketDataset(Dataset):\n",
    "    def __init__(self, buckets, keys=None):\n",
    "        buckets = {key: value for key, value in buckets.items() if value}\n",
    "        self.buckets_dict = buckets\n",
    "        \n",
    "        self.buckets = list(buckets.values())\n",
    "        # Flatten buckets\n",
    "        self.buckets_item = [item for bucket in self.buckets_dict.values() for item in bucket]\n",
    "        # print(self.buckets_item)\n",
    "        self.keys = keys\n",
    "        # filter buckets with in keys\n",
    "        if self.keys is not None:\n",
    "            self.buckets = [bucket for bucket in self.buckets if bucket[0][0] in keys]\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(bucket) for bucket in self.buckets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(idx)\n",
    "        node, b, c = self.buckets_item[idx]\n",
    "        # print(a)\n",
    "        # print(b.shape)\n",
    "        item = {'join_tables': node.info['join_tables'], \\\n",
    "                'plan_encode': b, \\\n",
    "                'att_encode': c, \\\n",
    "                'latency': node.info['latency'], \\\n",
    "                'cost': node.cost}\n",
    "        return item\n",
    "\n",
    "class BucketBatchSampler(Sampler):\n",
    "    def __init__(self, buckets, batch_size):\n",
    "        self.buckets = buckets\n",
    "        self.bucket_indices = list(range(len(buckets)))\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        random.shuffle(self.bucket_indices)\n",
    "        for i in range(0, len(self.bucket_indices), self.batch_size):\n",
    "            yield [item for bucket_idx in self.bucket_indices[i:i+self.batch_size] for item in range(sum(len(bucket) for bucket in self.buckets[:bucket_idx]), sum(len(bucket) for bucket in self.buckets[:bucket_idx+1]))]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.bucket_indices) // self.batch_size\n",
    "    \n",
    "buckets = exp\n",
    "keys = None\n",
    "\n",
    "# 创建数据集\n",
    "dataset = BucketDataset(buckets, keys)\n",
    "print(\"all eqset:\", len(dataset.buckets))\n",
    "# 创建BucketBatchSampler\n",
    "batch_sampler = BucketBatchSampler(dataset.buckets, batch_size=2)  # 这里的batch_size是桶的数量，每个批次包含2个桶的数据\n",
    "\n",
    "# 创建数据加载器\n",
    "dataloader = DataLoader(dataset, batch_sampler=batch_sampler)\n",
    "\n",
    "for batch in dataloader:\n",
    "    # print(batch['node'])\n",
    "    print(batch['plan_encode'][0].shape)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/wyz/miniconda3/envs/leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "import lightning.pytorch.callbacks as plc\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "from util import postgres\n",
    "from util import pg_executor\n",
    "from util import postgres\n",
    "from util import envs\n",
    "from util import plans_lib\n",
    "import pytorch_lightning.loggers as pl_loggers\n",
    "import pickle\n",
    "import math\n",
    "import json\n",
    "from test_case import SeqFormer\n",
    "from test_case import get_plan_encoding, configs, load_json, get_op_name_to_one_hot, plan_parameters, add_numerical_scalers\n",
    "from leon_experience import Experience\n",
    "import numpy as np\n",
    "import ray\n",
    "import time\n",
    "from argparse import ArgumentParser\n",
    "import copy\n",
    "import wandb\n",
    "import time\n",
    "import random\n",
    "DEVICE = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n",
    "# DEVICE = 'cpu'\n",
    "Transformer_model = SeqFormer(\n",
    "                        input_dim=configs['node_length'],\n",
    "                        hidden_dim=128,\n",
    "                        output_dim=1,\n",
    "                        mlp_activation=\"ReLU\",\n",
    "                        transformer_activation=\"gelu\",\n",
    "                        mlp_dropout=0.3,\n",
    "                        transformer_dropout=0.2,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PL_Leon(pl.LightningModule):\n",
    "    def __init__(self, model, optimizer_state_dict=None, learning_rate=0.001):\n",
    "        super(PL_Leon, self).__init__()\n",
    "        self.model = model\n",
    "        self.optimizer_state_dict = optimizer_state_dict\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "    def forward(self, plans, attns):\n",
    "        return self.model(plans, attns)[:, 0]\n",
    "\n",
    "    # def make_pairs(self, join_tables, calibrations: torch.tensor, costs, latency):\n",
    "    #     pairs = []\n",
    "    #     calibrations1 = []\n",
    "    #     calibrations2 = []\n",
    "    #     costs1 = []\n",
    "    #     costs2 = []\n",
    "    #     labels = []\n",
    "    #     for i in range(len(join_tables)):\n",
    "    #         for j in range(i + 1, len(join_tables)):\n",
    "    #             if join_tables[i] != join_tables[j]:\n",
    "    #                 continue\n",
    "    #             if latency[i] == latency[j]:\n",
    "    #                 continue\n",
    "    #             if latency[0] > latency[1]:\n",
    "    #                 label = 0\n",
    "    #             else:\n",
    "    #                 label = 1\n",
    "    #             labels.append(label)\n",
    "    #             costs1.append(costs[i])\n",
    "    #             costs2.append(costs[j])\n",
    "    #             calibrations1.append(calibrations[i])\n",
    "    #             calibrations2.append(calibrations[j])\n",
    "        \n",
    "        \n",
    "    #     # make tensor\n",
    "    #     labels = torch.tensor(labels, device=self.device)\n",
    "    #     costs1 = torch.tensor(costs1, device=self.device)\n",
    "    #     costs2 = torch.tensor(costs2, device=self.device)\n",
    "        \n",
    "    #     try:\n",
    "    #         calibrations1 = torch.stack(calibrations1)\n",
    "    #         calibrations2 = torch.stack(calibrations2)\n",
    "    #     except:\n",
    "    #         calibrations1 = None\n",
    "    #         calibrations2 = None\n",
    "    #     return labels, costs1, costs2, calibrations1, calibrations2\n",
    "    def make_pairs(self, join_tables, calibrations: torch.tensor, costs, latency):\n",
    "        # Find the boundaries where the join_table value changes\n",
    "        boundaries = [0] + [i for i in range(1, len(join_tables)) if join_tables[i] != join_tables[i-1]] + [len(join_tables)]\n",
    "\n",
    "        labels = []\n",
    "        costs1 = []\n",
    "        costs2 = []\n",
    "        calibrations1 = []\n",
    "        calibrations2 = []\n",
    "\n",
    "        for i in range(len(boundaries) - 1):\n",
    "            # Now join_tables[boundaries[i]:boundaries[i+1]] contains all indices for the same join_table value\n",
    "            for j in range(boundaries[i], boundaries[i+1]):\n",
    "                for k in range(j + 1, boundaries[i+1]):\n",
    "                    if latency[j] == latency[k]:\n",
    "                        continue\n",
    "                    if latency[j] > latency[k]:\n",
    "                        label = 0\n",
    "                    else:\n",
    "                        label = 1\n",
    "                    labels.append(label)\n",
    "                    costs1.append(costs[j])\n",
    "                    costs2.append(costs[k])\n",
    "                    calibrations1.append(calibrations[j])\n",
    "                    calibrations2.append(calibrations[k])\n",
    "\n",
    "        # make tensor\n",
    "        labels = torch.tensor(labels, device=self.device)\n",
    "        costs1 = torch.tensor(costs1, device=self.device)\n",
    "        costs2 = torch.tensor(costs2, device=self.device)\n",
    "\n",
    "        try:\n",
    "            calibrations1 = torch.stack(calibrations1)\n",
    "            calibrations2 = torch.stack(calibrations2)\n",
    "        except:\n",
    "            calibrations1 = None\n",
    "            calibrations2 = None\n",
    "        return labels, costs1, costs2, calibrations1, calibrations2\n",
    "\n",
    "    def get_pair_wise_loss(self, join_tables, calibration, costs, latency):\n",
    "\n",
    "        labels, costs1, costs2, calibrations1, calibrations2 = self.make_pairs(join_tables, calibration, costs, latency)\n",
    "        if calibrations1 is None or calibrations2 is None:\n",
    "            return None, None\n",
    "        loss_fn = nn.BCELoss()\n",
    "        calied_cost_1 = torch.log(costs1) * calibrations1\n",
    "        calied_cost_2 = torch.log(costs2) * calibrations2\n",
    "\n",
    "        sigmoid = F.sigmoid(-(calied_cost_1 - calied_cost_2))\n",
    "        loss = loss_fn(sigmoid, labels.double())\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            prediction = torch.round(sigmoid)\n",
    "            accuracy = torch.sum(prediction == labels).item() / len(labels)\n",
    "        return loss, accuracy\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.001)\n",
    "        if self.optimizer_state_dict is not None:\n",
    "            # Checks the params are the same.\n",
    "            # 'params': [139581476513104, ...]\n",
    "            curr = optimizer.state_dict()['param_groups'][0]['params']\n",
    "            prev = self.optimizer_state_dict['param_groups'][0]['params']\n",
    "            assert curr == prev, (curr, prev)\n",
    "            # print('Loading last iter\\'s optimizer state.')\n",
    "            # Prev optimizer state's LR may be stale.\n",
    "            optimizer.load_state_dict(self.optimizer_state_dict)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = self.learning_rate\n",
    "            assert optimizer.state_dict(\n",
    "            )['param_groups'][0]['lr'] == self.learning_rate\n",
    "            # print('LR', self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        plans = batch['plan_encode']\n",
    "        attns = batch['att_encode']\n",
    "        costs = batch['cost']\n",
    "        labels = batch['latency']\n",
    "        join_tables = batch['join_tables']\n",
    "        \n",
    "        # plans = torch.cat(plans, dim=0)\n",
    "        calibrations = self(plans, attns)\n",
    "        \n",
    "        loss, accuracy = self.get_pair_wise_loss(join_tables, calibrations, costs, labels)\n",
    "        \n",
    "        if loss:\n",
    "            self.log('train_loss', loss)\n",
    "            self.log('train_acc', accuracy)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | SeqFormer | 16.9 K\n",
      "------------------------------------\n",
      "16.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "16.9 K    Total params\n",
      "0.068     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53:  54%|█████▎    | 22/41 [01:05<00:56,  0.34it/s, v_num=zyfb]"
     ]
    }
   ],
   "source": [
    "def load_model(prev_optimizer_state_dict=None):\n",
    "    model = Transformer_model.to(DEVICE)\n",
    "    model = PL_Leon(model, prev_optimizer_state_dict)\n",
    "    return model\n",
    "\n",
    "def load_callbacks(logger):\n",
    "    callbacks = []\n",
    "    callbacks.append(plc.EarlyStopping(\n",
    "        monitor='v_acc',\n",
    "        mode='max',\n",
    "        patience=3,\n",
    "        min_delta=0.001\n",
    "    ))\n",
    "    if logger:\n",
    "        callbacks.append(plc.ModelCheckpoint(\n",
    "            dirpath= logger.experiment.dir,\n",
    "            monitor='val_scan',\n",
    "            filename='best-{epoch:02d}-{val_scan:.3f}',\n",
    "            save_top_k=1,\n",
    "            mode='min',\n",
    "            save_last=False\n",
    "        ))\n",
    "    return callbacks\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "# 获取上一级目录\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "logger =  pl_loggers.WandbLogger(save_dir=parent_directory + '/logs', name=\"exp_pair\", project='leon3')\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\",\n",
    "                        devices=[2],\n",
    "                        max_epochs=100,\n",
    "                        logger=logger)\n",
    "\n",
    "prev_optimizer_state_dict = None\n",
    "model = load_model().to(DEVICE)\n",
    "callbacks = load_callbacks(logger=None)\n",
    "# dataloader = DataLoader(dataset, batch_sampler=batch_sampler)\n",
    "dataloader_train = DataLoader(dataset, num_workers=0, batch_sampler=batch_sampler)\n",
    "# dataloader_val = DataLoader(dataloader, batch_size=2, shuffle=False, num_workers=0)\n",
    "trainer.fit(model, dataloader_train)\n",
    "prev_optimizer_state_dict = trainer.optimizers[0].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../log/exp.pkl\", 'wb') as f:\n",
    "            pickle.dump(exp, f) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
