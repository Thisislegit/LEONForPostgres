{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "import lightning.pytorch.callbacks as plc\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import pytorch_lightning.loggers as pl_loggers\n",
    "import pickle\n",
    "import math\n",
    "import json\n",
    "from test_case import configs\n",
    "import numpy as np\n",
    "import wandb\n",
    "import random\n",
    "DEVICE = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqFormer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dim,\n",
    "        output_dim,\n",
    "        mlp_activation=\"ReLU\",\n",
    "        transformer_activation=\"gelu\",\n",
    "        mlp_dropout=0.1,\n",
    "        transformer_dropout=0.1,\n",
    "    ):\n",
    "        super(SeqFormer, self).__init__()\n",
    "        # input_dim: node bits\n",
    "        self.node_length = input_dim\n",
    "        self.tranformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=32,\n",
    "                dim_feedforward=hidden_dim,\n",
    "                nhead=1,\n",
    "                batch_first=True,\n",
    "                activation=transformer_activation,\n",
    "                dropout=transformer_dropout,\n",
    "            ),\n",
    "            num_layers=1,\n",
    "        )\n",
    "        \n",
    "        if mlp_activation == \"ReLU\":\n",
    "            self.mlp_activation = nn.ReLU()\n",
    "        elif mlp_activation == \"GELU\":\n",
    "            self.mlp_activation = nn.GELU()\n",
    "        elif mlp_activation == \"LeakyReLU\":\n",
    "            self.mlp_activation = nn.LeakyReLU()\n",
    "        # self.mlp_hidden_dims = [128, 64, 32]\n",
    "        self.mlp_hidden_dims = [256, 128, 1]\n",
    "        self.embedding = nn.Linear(self.node_length, 32, bias=False)\n",
    "        self.mlp = nn.Sequential(\n",
    "            *[\n",
    "                nn.Linear(32, self.mlp_hidden_dims[0]),\n",
    "                nn.Dropout(mlp_dropout),\n",
    "                self.mlp_activation,\n",
    "                nn.Linear(self.mlp_hidden_dims[0], self.mlp_hidden_dims[1]),\n",
    "                nn.Dropout(mlp_dropout),\n",
    "                self.mlp_activation,\n",
    "                nn.Linear(self.mlp_hidden_dims[1], output_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.apply(self._init_weights)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # change x shape to (batch, seq_len, input_size) from (batch, len)\n",
    "        # one node is 18 bits\n",
    "        # x = x.view(x.shape[0], self.node_length, -1)\n",
    "        # x = x.transpose(1,2)\n",
    "        x = x.view(x.shape[0], -1, self.node_length)\n",
    "        x_in = self.embedding(x)\n",
    "        # attn_mask = attn_mask.repeat(4,1,1)\n",
    "        # attn_mask = attn_mask.unsqueeze(1).repeat(1, 3, 1, 1)\n",
    "        out = self.tranformer_encoder(x_in, mask=attn_mask)\n",
    "        # out = self.transformer_decoder(out, out, tgt_mask=attn_mask)\n",
    "        out = self.mlp(out)\n",
    "        # out = (torch.tanh(out).squeeze(dim=2) * 5).add(5) \n",
    "        # out = torch.tanh(out).squeeze(dim=2).add(1) * 5\n",
    "        out = torch.tanh(out).squeeze(dim=2).add(1)\n",
    "        return out # [0, 1] -> [1, 2] [??]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/wyz/miniconda3/envs/leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "Transformer_model = SeqFormer(\n",
    "                        input_dim=configs['node_length'],\n",
    "                        hidden_dim=256,\n",
    "                        output_dim=1,\n",
    "                        mlp_activation=\"ReLU\",\n",
    "                        transformer_activation=\"gelu\",\n",
    "                        mlp_dropout=0.1,\n",
    "                        transformer_dropout=0.1,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PL_Leon(pl.LightningModule):\n",
    "    def __init__(self, model, optimizer_state_dict=None, learning_rate=0.001):\n",
    "        super(PL_Leon, self).__init__()\n",
    "        self.model = model\n",
    "        self.optimizer_state_dict = optimizer_state_dict\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "    def forward(self, batch_pairs):\n",
    "        pass\n",
    "\n",
    "    def getBatchPairsLoss(self, labels, costs1, costs2, encoded_plans1, encoded_plans2, attns1, attns2):\n",
    "        \"\"\"\n",
    "        batch_pairs: a batch of train pairs\n",
    "        return. a batch of loss\n",
    "        \"\"\"\n",
    "        loss_fn = nn.BCELoss()\n",
    "        batsize = costs1.shape[0]\n",
    "        encoded_plans = torch.cat((encoded_plans1, encoded_plans2), dim=0)\n",
    "        attns = torch.cat((attns1, attns2), dim=0)\n",
    "        cali = self.model(encoded_plans, attns) # cali.shape [# of plan, pad_length] cali 是归一化后的基数估计\n",
    "        cali = cali[:, 0]\n",
    "        costs = torch.cat((costs1, costs2), dim=0)\n",
    "        calied_cost = torch.log(costs + 1) * cali\n",
    "        try:\n",
    "            sigmoid = F.sigmoid(-(calied_cost[:batsize] - calied_cost[batsize:]))\n",
    "            loss = loss_fn(sigmoid, labels.float())\n",
    "        except:\n",
    "            print(calied_cost, sigmoid)\n",
    "        with torch.no_grad():\n",
    "            prediction = torch.round(sigmoid)\n",
    "            accuracy = torch.sum(prediction == labels).item() / len(labels)\n",
    "        return loss, accuracy\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        labels, costs1, costs2, encoded_plans1, encoded_plans2, attns1, attns2, latency1, latency2 = batch\n",
    "        loss, acc  = self.getBatchPairsLoss(labels, costs1, costs2, encoded_plans1, encoded_plans2, attns1, attns2)\n",
    "        # loss = (torch.abs(latency1 - latency2) * loss / 90000).mean()\n",
    "        self.log_dict({'t_loss': loss, 't_acc': acc}, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        labels, costs1, costs2, encoded_plans1, encoded_plans2, attns1, attns2, latency1, latency2 = batch\n",
    "        loss, acc  = self.getBatchPairsLoss(labels, costs1, costs2, encoded_plans1, encoded_plans2, attns1, attns2)\n",
    "        # loss = (torch.abs(latency1 - latency2) * loss / 90000).mean()\n",
    "        self.log_dict({'v_loss': loss, 'v_acc': acc}, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.001)\n",
    "        if self.optimizer_state_dict is not None:\n",
    "            curr = optimizer.state_dict()['param_groups'][0]['params']\n",
    "            prev = self.optimizer_state_dict['param_groups'][0]['params']\n",
    "            assert curr == prev, (curr, prev)\n",
    "            optimizer.load_state_dict(self.optimizer_state_dict)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = self.learning_rate\n",
    "            assert optimizer.state_dict(\n",
    "            )['param_groups'][0]['lr'] == self.learning_rate\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(prev_optimizer_state_dict=None):\n",
    "    model = Transformer_model.to(DEVICE)\n",
    "    model = PL_Leon(model, prev_optimizer_state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeonDataset(Dataset):\n",
    "    def __init__(self, labels, costs1, costs2, encoded_plans1, encoded_plans2, attns1, attns2, latency1, latency2):\n",
    "        self.labels = labels\n",
    "        self.costs1 = costs1\n",
    "        self.costs2 = costs2\n",
    "        self.encoded_plans1 = encoded_plans1\n",
    "        self.encoded_plans2 = encoded_plans2\n",
    "        self.attns1 = attns1\n",
    "        self.attns2 = attns2\n",
    "        self.latency1 = latency1\n",
    "        self.latency2 = latency2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.labels[idx],\n",
    "                self.costs1[idx],\n",
    "                self.costs2[idx],\n",
    "                self.encoded_plans1[idx],\n",
    "                self.encoded_plans2[idx],\n",
    "                self.attns1[idx],\n",
    "                self.attns2[idx],\n",
    "                self.latency1[idx],\n",
    "                self.latency2[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(pairs):\n",
    "    labels = []\n",
    "    costs1 = []\n",
    "    costs2 = []\n",
    "    encoded_plans1 = []\n",
    "    encoded_plans2 = []\n",
    "    attns1 = []\n",
    "    attns2 = []\n",
    "    latency1 = []\n",
    "    latency2 = []\n",
    "    for pair in pairs:\n",
    "        if pair[0][0].info['latency'] > pair[1][0].info['latency']:\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "        labels.append(label)\n",
    "        costs1.append(pair[0][0].cost)\n",
    "        costs2.append(pair[1][0].cost)\n",
    "        encoded_plans1.append(pair[0][1])\n",
    "        encoded_plans2.append(pair[1][1])\n",
    "        attns1.append(pair[0][2])\n",
    "        attns2.append(pair[1][2])\n",
    "        latency1.append(pair[0][0].info['latency'])\n",
    "        latency2.append(pair[1][0].info['latency'])\n",
    "    labels = torch.tensor(labels)\n",
    "    costs1 = torch.tensor(costs1)\n",
    "    costs2 = torch.tensor(costs2)\n",
    "    encoded_plans1 = torch.stack(encoded_plans1)\n",
    "    encoded_plans2 = torch.stack(encoded_plans2)\n",
    "    attns1 = torch.stack(attns1)\n",
    "    attns2 = torch.stack(attns2)\n",
    "    latency1 = torch.tensor(latency1)\n",
    "    latency2 = torch.tensor(latency2)\n",
    "    dataset = LeonDataset(labels, costs1, costs2, encoded_plans1, encoded_plans2, attns1, attns2, latency1, latency2)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_callbacks(logger):\n",
    "    callbacks = []\n",
    "    callbacks.append(plc.EarlyStopping(\n",
    "        monitor='v_acc',\n",
    "        mode='max',\n",
    "        patience=5,\n",
    "        min_delta=0.001\n",
    "    ))\n",
    "    if logger:\n",
    "        callbacks.append(plc.ModelCheckpoint(\n",
    "            dirpath= logger.experiment.dir,\n",
    "            monitor='val_scan',\n",
    "            filename='best-{epoch:02d}-{val_scan:.3f}',\n",
    "            save_top_k=1,\n",
    "            mode='min',\n",
    "            save_last=False\n",
    "        ))\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Getpair(exp):\n",
    "    pairs = []\n",
    "    for eq in exp.keys():\n",
    "        for j in exp[eq]:\n",
    "            for k in exp[eq]:\n",
    "                if (j[0].info['sql_str'] == k[0].info['sql_str']) and (j[0].hint_str() == k[0].hint_str()): # sql 和 hint 都相同\n",
    "                    continue\n",
    "                # if (j[0].info['latency'] == k[0].info['latency']): # latency 相同 1s之内不把他train_pair\n",
    "                if max(j[0].info['latency'],k[0].info['latency']) / min(j[0].info['latency'],k[0].info['latency']) < 1.2:\n",
    "                    continue\n",
    "                # if j[0].info['latency'] == 90000 or k[0].info['latency'] == 90000:\n",
    "                #     continue\n",
    "                tem = [j, k]\n",
    "                pairs.append(tem)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../log/exp.pkl', 'rb') as f:\n",
    "        exp = pickle.load(f)\n",
    "current_directory = os.getcwd()\n",
    "# 获取上一级目录\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "logger =  pl_loggers.WandbLogger(save_dir=parent_directory + '/logs', name=\"embedding mlp\", project='leon3')\n",
    "prev_optimizer_state_dict = None\n",
    "callbacks = load_callbacks(logger=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 40])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model(torch.rand(512,40,18).to(DEVICE)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "cn,k,mc,mk,t 458\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "filtered_keys = [key for key in exp.keys() if len(key) > 10 and len(exp[key]) < 500 and len(exp[key]) > 457]\n",
    "print(len(filtered_keys))\n",
    "for eq in filtered_keys:\n",
    "    print(eq, len(exp[eq]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_pairs) 1228676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "train_pairs = Getpair(exp)\n",
    "print(\"len(train_pairs)\" ,len(train_pairs))\n",
    "leon_dataset = prepare_dataset(train_pairs)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\",\n",
    "                        devices=[2],\n",
    "                        max_epochs=40,\n",
    "                        logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwyz12234\u001b[0m (\u001b[33mleon1\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data1/wyz/online/LEONForPostgres/logs/wandb/run-20231223_045411-qtquud8u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leon1/leon3/runs/qtquud8u' target=\"_blank\">embedding mlp</a></strong> to <a href='https://wandb.ai/leon1/leon3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/leon1/leon3' target=\"_blank\">https://wandb.ai/leon1/leon3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/leon1/leon3/runs/qtquud8u' target=\"_blank\">https://wandb.ai/leon1/leon3/runs/qtquud8u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | SeqFormer | 63.1 K\n",
      "------------------------------------\n",
      "63.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "63.1 K    Total params\n",
      "0.252     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/wyz/miniconda3/envs/leon/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.5453, 12.5453, 12.5453,  ..., 11.4064, 11.8494, 11.5174],\n",
      "       device='cuda:2') tensor([0.3327, 0.2635, 0.0397, 0.0170, 0.0170, 0.2425, 0.9724, 0.9619, 0.7448,\n",
      "        0.5504, 0.5504, 0.9577, 0.5745, 0.4920, 0.1006, 0.0448, 0.0448, 0.4643,\n",
      "        0.9724, 0.9619, 0.7447, 0.5503, 0.5503, 0.9576, 0.9724, 0.9619, 0.7447,\n",
      "        0.5503, 0.5503, 0.9576, 0.9724, 0.9619, 0.7447, 0.5503, 0.5503, 0.9576,\n",
      "        0.5531, 0.4703, 0.0930, 0.0412, 0.0412, 0.4428, 0.5728, 0.4904, 0.1000,\n",
      "        0.0445, 0.0445, 0.4627, 0.9679, 0.9558, 0.7142, 0.5118, 0.5118, 0.9509,\n",
      "        0.9724, 0.9619, 0.7447, 0.5503, 0.5503, 0.9576, 0.9724, 0.9619, 0.7447,\n",
      "        0.5503, 0.5503, 0.9576, 0.9724, 0.9619, 0.7447, 0.5503, 0.5503, 0.9576,\n",
      "        0.9748, 0.9652, 0.7623, 0.5736, 0.5736, 0.9613, 0.5515, 0.4687, 0.0924,\n",
      "        0.0410, 0.0410, 0.4412, 0.9724, 0.9619, 0.7447, 0.5503, 0.5503, 0.9576,\n",
      "        0.3322, 0.2630, 0.0396, 0.0170, 0.0170, 0.2421, 0.9748, 0.9652, 0.7623,\n",
      "        0.5736, 0.5736, 0.9613, 0.9146, 0.8849, 0.4702, 0.2714, 0.2714, 0.8731,\n",
      "        0.9733, 0.9632, 0.7513, 0.5590, 0.5590, 0.9590, 0.6039, 0.5224, 0.1121,\n",
      "        0.0503, 0.0503, 0.4947, 0.9733, 0.9632, 0.7513, 0.5590, 0.5590, 0.9590,\n",
      "        0.3327, 0.2635, 0.0397, 0.0170, 0.0170, 0.2425, 0.6506, 0.5719, 0.1336,\n",
      "        0.0608, 0.0608, 0.5445, 0.9724, 0.9619, 0.7448, 0.5504, 0.5505, 0.9577,\n",
      "        0.6541, 0.5756, 0.1354, 0.0617, 0.0617, 0.5483, 0.9724, 0.9619, 0.7447,\n",
      "        0.5503, 0.5503, 0.9576, 0.9724, 0.9619, 0.7447, 0.5503, 0.5503, 0.9576,\n",
      "        0.9724, 0.9619, 0.7447, 0.5503, 0.5503, 0.9576, 0.9724, 0.9619, 0.7447,\n",
      "        0.5503, 0.5503, 0.9576, 0.9724, 0.9619, 0.7447, 0.5503, 0.5503, 0.9576,\n",
      "        0.6533, 0.5749, 0.1350, 0.0615, 0.0615, 0.5475, 0.9748, 0.9652, 0.7623,\n",
      "        0.5736, 0.5736, 0.9613, 0.6519, 0.5733, 0.1343, 0.0611, 0.0611, 0.5460,\n",
      "        0.9724, 0.9619, 0.7447, 0.5503, 0.5504, 0.9576, 0.9679, 0.9558, 0.7142,\n",
      "        0.5118, 0.5118, 0.9509, 0.9724, 0.9619, 0.7447, 0.5503, 0.5504, 0.9576,\n",
      "        0.9748, 0.9652, 0.7623, 0.5736, 0.5736, 0.9613, 0.9733, 0.9632, 0.7513,\n",
      "        0.5590, 0.5590, 0.9591, 0.9733, 0.9632, 0.7513, 0.5590, 0.5590, 0.9591,\n",
      "        0.6512, 0.5726, 0.1339, 0.0609, 0.0609, 0.5452, 0.6484, 0.5696, 0.1325,\n",
      "        0.0602, 0.0602, 0.5422, 0.9195, 0.8913, 0.4862, 0.2842, 0.2842, 0.8800,\n",
      "        0.9724, 0.9619, 0.7447, 0.5503, 0.5503, 0.9576, 0.6837, 0.6080, 0.1519,\n",
      "        0.0699, 0.0699, 0.5813, 0.6506, 0.5719, 0.1336, 0.0608, 0.0608, 0.5445,\n",
      "        0.9724, 0.9619, 0.7446, 0.5502, 0.5502, 0.9576, 0.6673, 0.0276, 0.4255,\n",
      "        0.0276, 0.0276, 0.0276, 0.4469, 0.4272, 0.0321, 0.0276, 0.0276, 0.0276,\n",
      "        0.0252, 0.4485, 0.0276, 0.6678, 0.0252, 0.0854, 0.0267, 0.3961, 0.0267,\n",
      "        0.6673, 0.3494, 0.0276, 0.3459, 0.0276, 0.0276, 0.0276, 0.0276, 0.0276,\n",
      "        0.3467, 0.0252, 0.3481, 0.0276, 0.0321, 0.0276, 0.0252, 0.0267, 0.0267,\n",
      "        0.3488, 0.3516, 0.0805, 0.0276, 0.3163, 0.3494, 0.0276, 0.5669, 0.0276,\n",
      "        0.3987, 0.0276, 0.0276, 0.0276, 0.0276, 0.0276, 0.4117, 0.0252, 0.4005,\n",
      "        0.0276, 0.0321, 0.0276, 0.0252, 0.0267, 0.0267, 0.4135, 0.5679, 0.0843,\n",
      "        0.3686, 0.5669, 0.5512, 0.0276, 0.3953, 0.0276, 0.0276, 0.0276, 0.0276,\n",
      "        0.0276, 0.4072, 0.0252, 0.3971, 0.0276, 0.0321, 0.0252, 0.0267, 0.0267,\n",
      "        0.4089, 0.5522, 0.0842, 0.3651, 0.5512, 0.0277, 0.4178, 0.6673, 0.0276,\n",
      "        0.4255, 0.0276, 0.0276, 0.4469, 0.4272, 0.0276, 0.0321, 0.0276, 0.0276,\n",
      "        0.0252, 0.0276, 0.4485, 0.0252, 0.6678, 0.0267, 0.0854, 0.0267, 0.3961,\n",
      "        0.6673, 0.0277, 0.3847, 0.5512, 0.0276, 0.3953, 0.0276, 0.0276, 0.0276,\n",
      "        0.0276, 0.0276, 0.4072, 0.0252, 0.3971, 0.0276, 0.0321, 0.0252, 0.0267,\n",
      "        0.0267, 0.4089, 0.5522, 0.0842, 0.3651, 0.5512, 0.0277, 0.0277, 0.0277,\n",
      "        0.0252, 0.0277, 0.0765, 0.0276, 0.0276, 0.0276, 0.0252, 0.0336, 0.0277,\n",
      "        0.0277, 0.0277, 0.0277, 0.0277, 0.0252, 0.0336, 0.0277, 0.0252, 0.0277,\n",
      "        0.0277, 0.0276, 0.0276, 0.0252, 0.0276, 0.3910, 0.0276, 0.3910, 0.0276,\n",
      "        0.0277, 0.0277, 0.0277, 0.4331, 0.3540, 0.0595, 0.0259, 0.0259, 0.3291,\n",
      "        0.9724, 0.9619, 0.7448, 0.5505, 0.5505, 0.9577, 0.6013, 0.5197, 0.1111,\n",
      "        0.0498, 0.0498, 0.4920, 0.9724, 0.9619, 0.7447, 0.5503, 0.5503, 0.9576,\n",
      "        0.9724, 0.9619, 0.7447, 0.5503, 0.5503, 0.9576, 0.9724, 0.9619, 0.7447,\n",
      "        0.5503, 0.5503, 0.9576, 0.9724, 0.9619, 0.7447, 0.5503, 0.5503, 0.9576,\n",
      "        0.9724, 0.9619, 0.7447, 0.5503, 0.5503, 0.9576, 0.5883, 0.5062, 0.1058,\n",
      "        0.0473, 0.0473, 0.4785, 0.9748, 0.9652, 0.7623, 0.5736, 0.5737, 0.9613,\n",
      "        0.5995, 0.5179, 0.1103, 0.0495, 0.0495, 0.4901, 0.9724, 0.9619],\n",
      "       device='cuda:2')\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'loss' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m dataloader_train \u001b[38;5;241m=\u001b[39m DataLoader(leon_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m dataloader_val \u001b[38;5;241m=\u001b[39m DataLoader(leon_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m prev_optimizer_state_dict \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39moptimizers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "File \u001b[0;32m~/miniconda3/envs/leon/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/leon/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/leon/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/leon/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/leon/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1033\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/miniconda3/envs/leon/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:1062\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1059\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1062\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/leon/lib/python3.8/site-packages/lightning/pytorch/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/leon/lib/python3.8/site-packages/lightning/pytorch/loops/evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/leon/lib/python3.8/site-packages/lightning/pytorch/loops/evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    385\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    390\u001b[0m )\n\u001b[0;32m--> 391\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/leon/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/leon/lib/python3.8/site-packages/lightning/pytorch/strategies/strategy.py:403\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 43\u001b[0m, in \u001b[0;36mPL_Leon.validation_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m     42\u001b[0m     labels, costs1, costs2, encoded_plans1, encoded_plans2, attns1, attns2, latency1, latency2 \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 43\u001b[0m     loss, acc  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetBatchPairsLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcosts1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcosts2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_plans1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_plans2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattns1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattns2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# loss = (torch.abs(latency1 - latency2) * loss / 90000).mean()\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_dict({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: acc}, on_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[4], line 32\u001b[0m, in \u001b[0;36mPL_Leon.getBatchPairsLoss\u001b[0;34m(self, labels, costs1, costs2, encoded_plans1, encoded_plans2, attns1, attns2)\u001b[0m\n\u001b[1;32m     30\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(sigmoid)\n\u001b[1;32m     31\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(prediction \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss\u001b[49m, accuracy\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'loss' referenced before assignment"
     ]
    }
   ],
   "source": [
    "dataloader_train = DataLoader(leon_dataset, batch_size=512, shuffle=True, num_workers=0)\n",
    "dataloader_val = DataLoader(leon_dataset, batch_size=512, shuffle=False, num_workers=0)\n",
    "trainer.fit(model, dataloader_train, dataloader_val)\n",
    "prev_optimizer_state_dict = trainer.optimizers[0].state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
